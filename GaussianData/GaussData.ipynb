{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import shap\n",
    "import torch\n",
    "import warnings\n",
    "from torch import nn\n",
    "from utils import generate_data\n",
    "from synthetic_dataset import SyntheticDataset\n",
    "from models import BaselineNet, MDN_Linear, MDN_Conv, SSN\n",
    "from metadatanorm import MetadataNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history of SSN with one-dimensional orthogonalisation trained for 500 epochs repeated 10 times\n",
    "SSN_200_train, SSN_200_val = [], []\n",
    "SSN_1000_train, SSN_1000_val = [], []\n",
    "SSN_2000_train, SSN_2000_val = [], []\n",
    "for i in range(1, 11):\n",
    "    SSN_200_train.append(np.load('experiments/same_test/cf1/SSN/batch_size200/run{}/d2000.npy'.format(i)))\n",
    "    SSN_200_val.append(np.load('experiments/same_test/cf1/SSN/batch_size200/run{}/val_acc_d2000.npy'.format(i)))\n",
    "    SSN_1000_train.append(np.load('experiments/same_test/cf1/SSN/batch_size1000/run{}/d2000.npy'.format(i)))\n",
    "    SSN_1000_val.append(np.load('experiments/same_test/cf1/SSN/batch_size1000/run{}/val_acc_d2000.npy'.format(i)))\n",
    "    SSN_2000_train.append(np.load('experiments/plot/SSN/batch_size2000/run{}/d2000.npy'.format(i)))\n",
    "    SSN_2000_val.append(np.load('experiments/plot/SSN/batch_size2000/run{}/val_acc_d2000.npy'.format(i)))\n",
    "df_SSN_200_train = pd.DataFrame(SSN_200_train).T\n",
    "df_SSN_200_val = pd.DataFrame(SSN_200_val).T\n",
    "df_SSN_1000_train = pd.DataFrame(SSN_1000_train).T\n",
    "df_SSN_1000_val = pd.DataFrame(SSN_1000_val).T\n",
    "df_SSN_2000_train = pd.DataFrame(SSN_2000_train).T\n",
    "df_SSN_2000_val = pd.DataFrame(SSN_2000_val).T\n",
    "\n",
    "# Plot training history of SSN with one-dimensional orthogonalisation trained for 500 epochs repeated 10 times\n",
    "fig, ax = plt.subplots(2, 3, sharey=True, figsize=(15, 8))\n",
    "sns.lineplot(ax=ax[0,0], data=df_SSN_200_train, palette=\"tab10\", linewidth=0.5, legend=False)\n",
    "sns.lineplot(ax=ax[1,0], data=df_SSN_200_val, palette=\"tab10\", linewidth=0.5, legend=False)\n",
    "sns.lineplot(ax=ax[0,1], data=df_SSN_1000_train, palette=\"tab10\", linewidth=0.5, legend=False)\n",
    "sns.lineplot(ax=ax[1,1], data=df_SSN_1000_val, palette=\"tab10\", linewidth=0.5, legend=False)\n",
    "sns.lineplot(ax=ax[0,2], data=df_SSN_2000_train, palette=\"tab10\", linewidth=0.5, legend=False)\n",
    "sns.lineplot(ax=ax[1,2], data=df_SSN_2000_val, palette=\"tab10\", linewidth=0.5, legend=False)\n",
    "ax[0,0].axhline(y=5/6*100, color='r', linestyle='--')\n",
    "ax[0,1].axhline(y=5/6*100, color='r', linestyle='--')\n",
    "ax[0,2].axhline(y=5/6*100, color='r', linestyle='--')\n",
    "ax[1,0].axhline(y=5/6*100, color='r', linestyle='--')\n",
    "ax[1,1].axhline(y=5/6*100, color='r', linestyle='--')\n",
    "ax[1,2].axhline(y=5/6*100, color='r', linestyle='--')\n",
    "ax[0,0].set_ylim(50, 100)\n",
    "ax[0,1].set_ylim(50, 100)\n",
    "ax[0,2].set_ylim(50, 100)\n",
    "ax[1,0].set_ylim(50, 100)\n",
    "ax[1,1].set_ylim(50, 100)\n",
    "ax[1,2].set_ylim(50, 100)\n",
    "ax[1,0].set_xlabel('Epochs')\n",
    "ax[1,1].set_xlabel('Epochs')\n",
    "ax[1,2].set_xlabel('Epochs')\n",
    "ax[0,0].set_ylabel('Accuracy (%)')\n",
    "ax[1,0].set_ylabel('Accuracy (%)')\n",
    "ax[0,0].set_title('Training Accuracy (Batch Size 200)')\n",
    "ax[0,1].set_title('Training Accuracy (Batch Size 1000)')\n",
    "ax[0,2].set_title('Training Accuracy (Batch Size 2000)')\n",
    "ax[1,0].set_title('Validation Accuracy (Batch Size 200)')\n",
    "ax[1,1].set_title('Validation Accuracy (Batch Size 1000)')\n",
    "ax[1,2].set_title('Validation Accuracy (Batch Size 2000)')\n",
    "plt.savefig('SSN_histories.pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and 95% confidence interval of test accuracies (accuracies for the individual runs can be copied from the skmetrics.txt and skmetrics_f.txt files))\n",
    "indices = [np.arange(8, 90, 9), np.arange(7, 89, 9)]\n",
    "files = ['experiments/final/Baseline/batch_size200/skmetrics.txt',\n",
    "         'experiments/final/Baseline/batch_size1000/skmetrics.txt',\n",
    "         'experiments/final/Baseline/batch_size2000/skmetrics.txt',\n",
    "         'experiments/final/Linear/batch_size200/skmetrics.txt',\n",
    "         'experiments/final/Linear/batch_size1000/skmetrics.txt',\n",
    "         'experiments/final/Linear/batch_size2000/skmetrics.txt',\n",
    "         'experiments/final/Conv/batch_size200/skmetrics.txt',\n",
    "         'experiments/final/Conv/batch_size1000/skmetrics.txt',\n",
    "         'experiments/final/Conv/batch_size2000/skmetrics.txt',\n",
    "         'experiments/final/cf1/SSN/batch_size200/skmetrics.txt',\n",
    "         'experiments/final/cf1/SSN/batch_size1000/skmetrics.txt',\n",
    "         'experiments/final/cf1/SSN/batch_size2000/skmetrics.txt',\n",
    "         'experiments/final/cf2/SSN/batch_size200/skmetrics.txt',\n",
    "         'experiments/final/cf2/SSN/batch_size1000/skmetrics.txt',\n",
    "         'experiments/final/cf2/SSN/batch_size2000/skmetrics.txt',\n",
    "         'experiments/final/Baseline/batch_size200/skmetrics_f.txt',\n",
    "         'experiments/final/Baseline/batch_size1000/skmetrics_f.txt',\n",
    "         'experiments/final/Baseline/batch_size2000/skmetrics_f.txt',\n",
    "         'experiments/final/Linear/batch_size200/skmetrics_f.txt',\n",
    "         'experiments/final/Linear/batch_size1000/skmetrics_f.txt',\n",
    "         'experiments/final/Linear/batch_size2000/skmetrics_f.txt',\n",
    "         'experiments/final/Conv/batch_size200/skmetrics_f.txt',\n",
    "         'experiments/final/Conv/batch_size1000/skmetrics_f.txt',\n",
    "         'experiments/final/Conv/batch_size2000/skmetrics_f.txt',\n",
    "         'experiments/final/cf1/SSN/batch_size200/skmetrics_f.txt',\n",
    "         'experiments/final/cf1/SSN/batch_size1000/skmetrics_f.txt',\n",
    "         'experiments/final/cf1/SSN/batch_size2000/skmetrics_f.txt',\n",
    "         'experiments/final/cf2/SSN/batch_size200/skmetrics_f.txt',\n",
    "         'experiments/final/cf2/SSN/batch_size1000/skmetrics_f.txt',\n",
    "         'experiments/final/cf2/SSN/batch_size2000/skmetrics_f.txt']\n",
    "files_new = []\n",
    "for i in range(len(files)//2):\n",
    "    files_new.append(files[i])\n",
    "    files_new.append(files[i+len(files)//2])  \n",
    "results = {'Baseline_200_A': [], 'Baseline_200_B': [], 'Baseline_1000_A': [], 'Baseline_1000_B': [], 'Baseline_2000_A': [], 'Baseline_2000_B': [],\n",
    "           'Linear_200_A': [], 'Linear_200_B': [], 'Linear_1000_A': [], 'Linear_1000_B': [], 'Linear_2000_A': [], 'Linear_2000_B': [],\n",
    "           'Conv_200_A': [], 'Conv_200_B': [], 'Conv_1000_A': [], 'Conv_1000_B': [], 'Conv_2000_A': [], 'Conv_2000_B': [],\n",
    "           'SSN1_200_A': [], 'SSN1_200_B': [], 'SSN1_1000_A': [], 'SSN1_1000_B': [], 'SSN1_2000_A': [], 'SSN1_2000_B': [],\n",
    "           'SSN2_200_A': [], 'SSN2_200_B': [], 'SSN2_1000_A': [], 'SSN2_1000_B': [], 'SSN2_2000_A': [], 'SSN2_2000_B': []}\n",
    "for i, key in enumerate(results.keys()):\n",
    "    file = open(files_new[i], 'r')\n",
    "    j = 0\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        if i % 2 == 0:\n",
    "            if j in indices[0]:\n",
    "                results[key].append(float(line.split()[2]))\n",
    "        else:\n",
    "            if j in indices[1]:\n",
    "                results[key].append(float(line.split()[2]))\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add PMDN results (run PMDN train.py file from command line to obtain results)\n",
    "results['PMDN_200_A'] = [0.8885, 0.814, 0.6105, 0.9065, 0.9215, 0.881, 0.827, 0.8335, 0.9095, 0.6025]\n",
    "results['PMDN_200_B'] = [0.7205, 0.728, 0.553, 0.7085, 0.626, 0.7725, 0.6815, 0.7555, 0.687, 0.573]\n",
    "results['PMDN_1000_A'] = [0.7945, 0.789, 0.945, 0.7855, 0.897, 0.8275, 0.9215, 0.8255, 0.756, 0.5845]\n",
    "results['PMDN_1000_B'] = [0.624, 0.645, 0.488, 0.602, 0.6335, 0.6135, 0.6135, 0.6055, 0.5785, 0.571]\n",
    "results['PMDN_2000_A'] = [0.8155, 0.8585, 0.9385, 0.9415, 0.6095, 0.937, 0.844, 0.782, 0.8225, 0.627]\n",
    "results['PMDN_2000_B'] = [0.478, 0.5025, 0.427, 0.455, 0.5165, 0.401, 0.4595, 0.5255, 0.625, 0.51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and 95% confidence interval of test accuracies stored in results dictionary and store them in a pandas dataframe\n",
    "df = pd.DataFrame.from_dict(results)\n",
    "df_mean = df.mean()\n",
    "df_std = df.std()\n",
    "df_conf = df_std/np.sqrt(len(df))\n",
    "df_conf = 1.96*df_conf\n",
    "df_mean = df_mean.to_frame()\n",
    "df_mean.columns = ['mean']\n",
    "df_conf = df_conf.to_frame()\n",
    "df_conf.columns = ['conf']\n",
    "# Concatenate mean and 95% confidence interval in one dataframe\n",
    "df_mean_conf = pd.concat([df_mean, df_conf], axis=1)\n",
    "df_mean_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training and test data for estimation of SHAP values\n",
    "device = torch.device(\"cuda:1\")\n",
    "seed = 1234\n",
    "N = 1000\n",
    "labels, cf, _, _, x, y = generate_data(N, seed=seed)\n",
    "labels_val, cf_val, _, _, x_val, y_val = generate_data(N, seed=seed+1)\n",
    "x = np.swapaxes(x, 1, 3) # move channels after batch so we have (N, channels, h, w)\n",
    "x_val = np.swapaxes(x_val, 1, 3)\n",
    "trainset_size = 2 * N\n",
    "X = np.zeros((N*2,3))\n",
    "X[:,0] = labels\n",
    "X[:,1] = cf\n",
    "X[:,2] = np.ones((N*2,))\n",
    "XTX = np.transpose(X).dot(X)\n",
    "kernel = np.linalg.inv(XTX)\n",
    "cf_kernel = nn.Parameter(torch.tensor(kernel).float().to(device), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "# Batch size 200\n",
    "batch_size = 200\n",
    "train_set = SyntheticDataset(x, labels, cf)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_set = SyntheticDataset(x_val, labels_val, cf_val)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set,\n",
    "    batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "train_batch = next(iter(train_loader))\n",
    "train_data = train_batch['image'].float().to(device)\n",
    "train_target = train_batch['label'].float().to(device)\n",
    "train_cf_batch = train_batch['cfs'].float().to(device)\n",
    "test_batch = next(iter(val_loader))\n",
    "test_data = test_batch['image'].float().to(device)\n",
    "test_target = test_batch['label'].float().to(device)\n",
    "test_cf_batch = test_batch['cfs'].float().to(device)\n",
    "\n",
    "# Calculate SHAP values for Baseline\n",
    "Baseline_200_SHAP = 0\n",
    "for i in range(1, 6):\n",
    "    model = BaselineNet().to(device)\n",
    "    model.load_state_dict(torch.load(f'plot_data/experiments/final/Baseline/batch_size200/run{i}/best_model.pth'))\n",
    "    model.eval()\n",
    "    explainer = shap.DeepExplainer(model, train_data)\n",
    "    shap_values = explainer.shap_values(test_data)\n",
    "    for i in range(shap_values.shape[0]):\n",
    "        Baseline_200_SHAP += np.abs(shap_values[i,:,:16,16:]).mean()\n",
    "# MDN Linear\n",
    "Linear_200_SHAP = 0\n",
    "for i in range(1, 6):\n",
    "    model = MDN_Linear(2*N, batch_size, cf_kernel).to(device)\n",
    "    model.load_state_dict(torch.load(f'plot_data/experiments/final/Linear/batch_size200/run{i}/best_model.pth'))\n",
    "    model.eval()\n",
    "    X_batch = np.zeros((batch_size,3))\n",
    "    X_batch[:,0] = train_target.cpu().detach().numpy()\n",
    "    X_batch[:,1] = train_cf_batch.cpu().detach().numpy()\n",
    "    X_batch[:,2] = np.ones((batch_size,))\n",
    "    with torch.no_grad():\n",
    "        model.cfs = nn.Parameter(torch.Tensor(X_batch).to(device), requires_grad=False)\n",
    "    explainer = shap.DeepExplainer(model, train_data)\n",
    "    X_batch_test = np.zeros((batch_size,3))\n",
    "    X_batch_test[:,0] = test_target.cpu().detach().numpy()\n",
    "    X_batch_test[:,1] = test_cf_batch.cpu().detach().numpy()\n",
    "    X_batch_test[:,2] = np.ones((batch_size,))\n",
    "    explainer.explainer.model.cfs = nn.Parameter(torch.cat((torch.Tensor(X_batch).to(device), torch.Tensor(X_batch_test).to(device))), requires_grad=False)\n",
    "    shap_values = explainer.shap_values(test_data)\n",
    "    for i in range(shap_values.shape[0]):\n",
    "        Linear_200_SHAP += np.abs(shap_values[i,:,:16,16:]).mean()   \n",
    "# MDN Conv\n",
    "Conv_200_SHAP = 0\n",
    "for i in range(1, 6):\n",
    "    model = MDN_Conv(2*N, batch_size, cf_kernel).to(device)\n",
    "    model.load_state_dict(torch.load(f'plot_data/experiments/final/Conv/batch_size200/run{i}/best_model.pth'))\n",
    "    model.eval()\n",
    "    X_batch = np.zeros((batch_size,3))\n",
    "    X_batch[:,0] = train_target.cpu().detach().numpy()\n",
    "    X_batch[:,1] = train_cf_batch.cpu().detach().numpy()\n",
    "    X_batch[:,2] = np.ones((batch_size,))\n",
    "    with torch.no_grad():\n",
    "        model.cfs = nn.Parameter(torch.Tensor(X_batch).to(device), requires_grad=False)\n",
    "    explainer = shap.DeepExplainer(model, train_data)\n",
    "    X_batch_test = np.zeros((batch_size,3))\n",
    "    X_batch_test[:,0] = test_target.cpu().detach().numpy()\n",
    "    X_batch_test[:,1] = test_cf_batch.cpu().detach().numpy()\n",
    "    X_batch_test[:,2] = np.ones((batch_size,))\n",
    "    explainer.explainer.model.cfs = nn.Parameter(torch.cat((torch.Tensor(X_batch).to(device), torch.Tensor(X_batch_test).to(device))), requires_grad=False)\n",
    "    shap_values = explainer.shap_values(test_data)\n",
    "    for i in range(shap_values.shape[0]):\n",
    "        Conv_200_SHAP += np.abs(shap_values[i,:,:16,16:]).mean()       \n",
    "# SSN1\n",
    "SSN1_200_SHAP = 0\n",
    "for i in range(1, 6):\n",
    "    model = SSN(batch_size, cf_dim=1, num_features=32).to(device)\n",
    "    model.load_state_dict(torch.load(f'plot_data/experiments/final/cf1/SSN/batch_size200/run{i}/best_model.pth'))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.cfs = nn.Parameter(torch.Tensor(train_cf_batch.float())[:,None], requires_grad=False)\n",
    "    explainer = shap.DeepExplainer(model, train_data)\n",
    "    explainer.explainer.model.cfs = nn.Parameter(torch.cat((torch.Tensor(train_cf_batch.float()), torch.Tensor(test_cf_batch.float())))[:,None], requires_grad=False)\n",
    "    shap_values = explainer.shap_values(test_data)\n",
    "    for i in range(shap_values.shape[0]):\n",
    "        SSN1_200_SHAP += np.abs(shap_values[i,:,:16,16:]).mean()\n",
    "# SSN2\n",
    "SSN2_200_SHAP = 0\n",
    "for i in range(1, 6):\n",
    "    model = SSN(batch_size, cf_dim=2, num_features=32).to(device)\n",
    "    model.load_state_dict(torch.load(f'plot_data/experiments/final/cf2/SSN/batch_size200/run{i}/best_model.pth'))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.cfs = nn.Parameter(torch.cat((torch.ones(len(train_cf_batch), 1).to(device), torch.Tensor(train_cf_batch.float())[:,None].to(device)), dim=1).to(device), requires_grad=False)\n",
    "    explainer = shap.DeepExplainer(model, train_data)\n",
    "    explainer.explainer.model.cfs = nn.Parameter(torch.cat((torch.cat((torch.ones(len(train_cf_batch), 1).to(device), torch.Tensor(train_cf_batch.float())[:,None].to(device)), dim=1), \n",
    "                                                           torch.cat((torch.ones(len(test_cf_batch), 1).to(device), torch.Tensor(test_cf_batch.float())[:,None].to(device)), dim=1)), dim=0).to(device), requires_grad=False)\n",
    "    shap_values = explainer.shap_values(test_data)\n",
    "    for i in range(shap_values.shape[0]):\n",
    "        SSN2_200_SHAP += np.abs(shap_values[i,:,:16,16:]).mean()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Baseline SHAP: ', Baseline_200_SHAP/5)\n",
    "print('Linear SHAP: ', Linear_200_SHAP/5)\n",
    "print('Conv SHAP: ', Conv_200_SHAP/5)\n",
    "print('SSN1 SHAP: ', SSN1_200_SHAP/5)\n",
    "print('SSN2 SHAP: ', SSN2_200_SHAP/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "# Batch size 1000\n",
    "batch_size = 1000\n",
    "train_set = SyntheticDataset(x, labels, cf)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_set = SyntheticDataset(x_val, labels_val, cf_val)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set,\n",
    "    batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "train_batch = next(iter(train_loader))\n",
    "train_data = train_batch['image'].float().to(device)\n",
    "train_target = train_batch['label'].float().to(device)\n",
    "train_cf_batch = train_batch['cfs'].float().to(device)\n",
    "test_batch = next(iter(val_loader))\n",
    "test_data = test_batch['image'].float().to(device)\n",
    "test_target = test_batch['label'].float().to(device)\n",
    "test_cf_batch = test_batch['cfs'].float().to(device)\n",
    "\n",
    "# Calculate SHAP values for Baseline\n",
    "Baseline_1000_SHAP = 0\n",
    "for i in range(1, 6):\n",
    "    model = BaselineNet().to(device)\n",
    "    model.load_state_dict(torch.load(f'plot_data/experiments/final/Baseline/batch_size1000/run{i}/best_model.pth'))\n",
    "    model.eval()\n",
    "    explainer = shap.DeepExplainer(model, train_data)\n",
    "    shap_values = explainer.shap_values(test_data)\n",
    "    for i in range(shap_values.shape[0]):\n",
    "        Baseline_1000_SHAP += np.abs(shap_values[i,:,:16,16:]).mean()\n",
    "# MDN Linear\n",
    "Linear_1000_SHAP = 0\n",
    "for i in range(1, 6):\n",
    "    model = MDN_Linear(2*N, batch_size, cf_kernel).to(device)\n",
    "    model.load_state_dict(torch.load(f'plot_data/experiments/final/Linear/batch_size1000/run{i}/best_model.pth'))\n",
    "    model.eval()\n",
    "    X_batch = np.zeros((batch_size,3))\n",
    "    X_batch[:,0] = train_target.cpu().detach().numpy()\n",
    "    X_batch[:,1] = train_cf_batch.cpu().detach().numpy()\n",
    "    X_batch[:,2] = np.ones((batch_size,))\n",
    "    with torch.no_grad():\n",
    "        model.cfs = nn.Parameter(torch.Tensor(X_batch).to(device), requires_grad=False)\n",
    "    explainer = shap.DeepExplainer(model, train_data)\n",
    "    X_batch_test = np.zeros((batch_size,3))\n",
    "    X_batch_test[:,0] = test_target.cpu().detach().numpy()\n",
    "    X_batch_test[:,1] = test_cf_batch.cpu().detach().numpy()\n",
    "    X_batch_test[:,2] = np.ones((batch_size,))\n",
    "    explainer.explainer.model.cfs = nn.Parameter(torch.cat((torch.Tensor(X_batch).to(device), torch.Tensor(X_batch_test).to(device))), requires_grad=False)\n",
    "    shap_values = explainer.shap_values(test_data)\n",
    "    for i in range(shap_values.shape[0]):\n",
    "        Linear_1000_SHAP += np.abs(shap_values[i,:,:16,16:]).mean()   \n",
    "# MDN Conv\n",
    "Conv_1000_SHAP = 0\n",
    "for i in range(1, 6):\n",
    "    model = MDN_Conv(2*N, batch_size, cf_kernel).to(device)\n",
    "    model.load_state_dict(torch.load(f'plot_data/experiments/final/Conv/batch_size1000/run{i}/best_model.pth'))\n",
    "    model.eval()\n",
    "    X_batch = np.zeros((batch_size,3))\n",
    "    X_batch[:,0] = train_target.cpu().detach().numpy()\n",
    "    X_batch[:,1] = train_cf_batch.cpu().detach().numpy()\n",
    "    X_batch[:,2] = np.ones((batch_size,))\n",
    "    with torch.no_grad():\n",
    "        model.cfs = nn.Parameter(torch.Tensor(X_batch).to(device), requires_grad=False)\n",
    "    explainer = shap.DeepExplainer(model, train_data)\n",
    "    X_batch_test = np.zeros((batch_size,3))\n",
    "    X_batch_test[:,0] = test_target.cpu().detach().numpy()\n",
    "    X_batch_test[:,1] = test_cf_batch.cpu().detach().numpy()\n",
    "    X_batch_test[:,2] = np.ones((batch_size,))\n",
    "    explainer.explainer.model.cfs = nn.Parameter(torch.cat((torch.Tensor(X_batch).to(device), torch.Tensor(X_batch_test).to(device))), requires_grad=False)\n",
    "    shap_values = explainer.shap_values(test_data)\n",
    "    for i in range(shap_values.shape[0]):\n",
    "        Conv_1000_SHAP += np.abs(shap_values[i,:,:16,16:]).mean()       \n",
    "# SSN1\n",
    "SSN1_1000_SHAP = 0\n",
    "for i in range(1, 6):\n",
    "    model = SSN(batch_size, cf_dim=1, num_features=32).to(device)\n",
    "    model.load_state_dict(torch.load(f'plot_data/experiments/final/cf1/SSN/batch_size1000/run{i}/best_model.pth'))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.cfs = nn.Parameter(torch.Tensor(train_cf_batch.float())[:,None], requires_grad=False)\n",
    "    explainer = shap.DeepExplainer(model, train_data)\n",
    "    explainer.explainer.model.cfs = nn.Parameter(torch.cat((torch.Tensor(train_cf_batch.float()), torch.Tensor(test_cf_batch.float())))[:,None], requires_grad=False)\n",
    "    shap_values = explainer.shap_values(test_data)\n",
    "    for i in range(shap_values.shape[0]):\n",
    "        SSN1_1000_SHAP += np.abs(shap_values[i,:,:16,16:]).mean()\n",
    "# SSN2\n",
    "SSN2_1000_SHAP = 0\n",
    "for i in range(1, 6):\n",
    "    model = SSN(batch_size, cf_dim=2, num_features=32).to(device)\n",
    "    model.load_state_dict(torch.load(f'plot_data/experiments/final/cf2/SSN/batch_size1000/run{i}/best_model.pth'))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.cfs = nn.Parameter(torch.cat((torch.ones(len(train_cf_batch), 1).to(device), torch.Tensor(train_cf_batch.float())[:,None].to(device)), dim=1).to(device), requires_grad=False)\n",
    "    explainer = shap.DeepExplainer(model, train_data)\n",
    "    explainer.explainer.model.cfs = nn.Parameter(torch.cat((torch.cat((torch.ones(len(train_cf_batch), 1).to(device), torch.Tensor(train_cf_batch.float())[:,None].to(device)), dim=1), \n",
    "                                                           torch.cat((torch.ones(len(test_cf_batch), 1).to(device), torch.Tensor(test_cf_batch.float())[:,None].to(device)), dim=1)), dim=0).to(device), requires_grad=False)\n",
    "    shap_values = explainer.shap_values(test_data)\n",
    "    for i in range(shap_values.shape[0]):\n",
    "        SSN2_1000_SHAP += np.abs(shap_values[i,:,:16,16:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Baseline SHAP: ', Baseline_1000_SHAP/5/5)\n",
    "print('Linear SHAP: ', Linear_1000_SHAP/5/5)\n",
    "print('Conv SHAP: ', Conv_1000_SHAP/5/5)\n",
    "print('SSN1 SHAP: ', SSN1_1000_SHAP/5/5)\n",
    "print('SSN2 SHAP: ', SSN2_1000_SHAP/5/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "# Batch size 2000\n",
    "batch_size = 2000\n",
    "train_set = SyntheticDataset(x, labels, cf)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_set = SyntheticDataset(x_val, labels_val, cf_val)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set,\n",
    "    batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "train_batch = next(iter(train_loader))\n",
    "train_data = train_batch['image'].float().to(device)\n",
    "train_target = train_batch['label'].float().to(device)\n",
    "train_cf_batch = train_batch['cfs'].float().to(device)\n",
    "test_batch = next(iter(val_loader))\n",
    "test_data = test_batch['image'].float().to(device)\n",
    "test_target = test_batch['label'].float().to(device)\n",
    "test_cf_batch = test_batch['cfs'].float().to(device)\n",
    "\n",
    "# Calculate SHAP values for Baseline\n",
    "Baseline_2000_SHAP = 0\n",
    "for i in range(1, 6):\n",
    "    model = BaselineNet().to(device)\n",
    "    model.load_state_dict(torch.load(f'plot_data/experiments/final/Baseline/batch_size2000/run{i}/best_model.pth'))\n",
    "    model.eval()\n",
    "    explainer = shap.DeepExplainer(model, train_data)\n",
    "    shap_values = explainer.shap_values(test_data)\n",
    "    for i in range(shap_values.shape[0]):\n",
    "        Baseline_2000_SHAP += np.abs(shap_values[i,:,:16,16:]).mean()\n",
    "# MDN Linear\n",
    "Linear_2000_SHAP = 0\n",
    "for i in range(1, 6):\n",
    "    model = MDN_Linear(2*N, batch_size, cf_kernel).to(device)\n",
    "    model.load_state_dict(torch.load(f'plot_data/experiments/final/Linear/batch_size2000/run{i}/best_model.pth'))\n",
    "    model.eval()\n",
    "    X_batch = np.zeros((batch_size,3))\n",
    "    X_batch[:,0] = train_target.cpu().detach().numpy()\n",
    "    X_batch[:,1] = train_cf_batch.cpu().detach().numpy()\n",
    "    X_batch[:,2] = np.ones((batch_size,))\n",
    "    with torch.no_grad():\n",
    "        model.cfs = nn.Parameter(torch.Tensor(X_batch).to(device), requires_grad=False)\n",
    "    explainer = shap.DeepExplainer(model, train_data)\n",
    "    X_batch_test = np.zeros((batch_size,3))\n",
    "    X_batch_test[:,0] = test_target.cpu().detach().numpy()\n",
    "    X_batch_test[:,1] = test_cf_batch.cpu().detach().numpy()\n",
    "    X_batch_test[:,2] = np.ones((batch_size,))\n",
    "    explainer.explainer.model.cfs = nn.Parameter(torch.cat((torch.Tensor(X_batch).to(device), torch.Tensor(X_batch_test).to(device))), requires_grad=False)\n",
    "    shap_values = explainer.shap_values(test_data)\n",
    "    for i in range(shap_values.shape[0]):\n",
    "        Linear_2000_SHAP += np.abs(shap_values[i,:,:16,16:]).mean()   \n",
    "# MDN Conv\n",
    "Conv_2000_SHAP = 0\n",
    "for i in range(1, 6):\n",
    "    model = MDN_Conv(2*N, batch_size, cf_kernel).to(device)\n",
    "    model.load_state_dict(torch.load(f'plot_data/experiments/final/Conv/batch_size2000/run{i}/best_model.pth'))\n",
    "    model.eval()\n",
    "    X_batch = np.zeros((batch_size,3))\n",
    "    X_batch[:,0] = train_target.cpu().detach().numpy()\n",
    "    X_batch[:,1] = train_cf_batch.cpu().detach().numpy()\n",
    "    X_batch[:,2] = np.ones((batch_size,))\n",
    "    with torch.no_grad():\n",
    "        model.cfs = nn.Parameter(torch.Tensor(X_batch).to(device), requires_grad=False)\n",
    "    explainer = shap.DeepExplainer(model, train_data)\n",
    "    X_batch_test = np.zeros((batch_size,3))\n",
    "    X_batch_test[:,0] = test_target.cpu().detach().numpy()\n",
    "    X_batch_test[:,1] = test_cf_batch.cpu().detach().numpy()\n",
    "    X_batch_test[:,2] = np.ones((batch_size,))\n",
    "    explainer.explainer.model.cfs = nn.Parameter(torch.cat((torch.Tensor(X_batch).to(device), torch.Tensor(X_batch_test).to(device))), requires_grad=False)\n",
    "    shap_values = explainer.shap_values(test_data)\n",
    "    for i in range(shap_values.shape[0]):\n",
    "        Conv_2000_SHAP += np.abs(shap_values[i,:,:16,16:]).mean()       \n",
    "# SSN1\n",
    "SSN1_2000_SHAP = 0\n",
    "for i in range(1, 6):\n",
    "    model = SSN(batch_size, cf_dim=1, num_features=32).to(device)\n",
    "    model.load_state_dict(torch.load(f'plot_data/experiments/final/cf1/SSN/batch_size2000/run{i}/best_model.pth'))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.cfs = nn.Parameter(torch.Tensor(train_cf_batch.float())[:,None], requires_grad=False)\n",
    "    explainer = shap.DeepExplainer(model, train_data)\n",
    "    explainer.explainer.model.cfs = nn.Parameter(torch.cat((torch.Tensor(train_cf_batch.float()), torch.Tensor(test_cf_batch.float())))[:,None], requires_grad=False)\n",
    "    shap_values = explainer.shap_values(test_data)\n",
    "    for i in range(shap_values.shape[0]):\n",
    "        SSN1_2000_SHAP += np.abs(shap_values[i,:,:16,16:]).mean()\n",
    "# SSN2\n",
    "SSN2_2000_SHAP = 0\n",
    "for i in range(1, 6):\n",
    "    model = SSN(batch_size, cf_dim=2, num_features=32).to(device)\n",
    "    model.load_state_dict(torch.load(f'plot_data/experiments/final/cf2/SSN/batch_size2000/run{i}/best_model.pth'))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.cfs = nn.Parameter(torch.cat((torch.ones(len(train_cf_batch), 1).to(device), torch.Tensor(train_cf_batch.float())[:,None].to(device)), dim=1).to(device), requires_grad=False)\n",
    "    explainer = shap.DeepExplainer(model, train_data)\n",
    "    explainer.explainer.model.cfs = nn.Parameter(torch.cat((torch.cat((torch.ones(len(train_cf_batch), 1).to(device), torch.Tensor(train_cf_batch.float())[:,None].to(device)), dim=1), \n",
    "                                                           torch.cat((torch.ones(len(test_cf_batch), 1).to(device), torch.Tensor(test_cf_batch.float())[:,None].to(device)), dim=1)), dim=0).to(device), requires_grad=False)\n",
    "    shap_values = explainer.shap_values(test_data)\n",
    "    for i in range(shap_values.shape[0]):\n",
    "        SSN2_2000_SHAP += np.abs(shap_values[i,:,:16,16:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Baseline SHAP: ', Baseline_2000_SHAP/5/10)\n",
    "print('Linear SHAP: ', Linear_2000_SHAP/5/10)\n",
    "print('Conv SHAP: ', Conv_2000_SHAP/5/10)\n",
    "print('SSN1 SHAP: ', SSN1_2000_SHAP/5/10)\n",
    "print('SSN2 SHAP: ', SSN2_2000_SHAP/5/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
