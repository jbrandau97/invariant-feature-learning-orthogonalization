{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook for the Medical Data experiment\n",
    "\n",
    "Sources of data: \\\n",
    "Guangzhou: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia/versions/1?resource=download&select=chest_xray \\\n",
    "RSNA: https://www.kaggle.com/competitions/rsna-pneumonia-detection-challenge/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import torch \n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pydicom\n",
    "import os\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR, ConstantLR\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from torchview import draw_graph\n",
    "from torchmetrics.classification import BinaryAUROC, MulticlassAUROC\n",
    "\n",
    "from models import Baseline, SemiStructuredNet\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\") # GPU\n",
    "downloads_path = str(Path.home() / \"Downloads\") # Downloads folder\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:734\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the data directory for the adult dataset\n",
    "create_rsna = False # Set marker for creating the RSNA data directory\n",
    "if create_rsna:\n",
    "    # Read label file\n",
    "    labels = pd.read_csv(downloads_path + \"/rsna-pneumonia-detection-challenge/stage_2_train_labels.csv\")[['patientId','Target']]\n",
    "    # Define input path and output paths\n",
    "    input_path = downloads_path + \"/rsna-pneumonia-detection-challenge/stage_2_train_images/\"\n",
    "    if not os.path.exists(downloads_path + \"/rsna-pneumonia-detection-challenge/train/\"):\n",
    "        os.mkdir(downloads_path + \"/rsna-pneumonia-detection-challenge/train/\") # Create the new train folder if it does not exist\n",
    "    output_path_normal = downloads_path + \"/rsna-pneumonia-detection-challenge/train/NORMAL/\"\n",
    "    output_path_pneumonia = downloads_path + \"/rsna-pneumonia-detection-challenge/train/PNEUMONIA/\"\n",
    "    # Create the new normal and pneumonia folders if they do not exist\n",
    "    if (not os.path.exists(output_path_normal)) and (not os.path.exists(output_path_pneumonia)):\n",
    "        os.mkdir(output_path_normal) # Create the new normal folder if it does not exist\n",
    "        os.mkdir(output_path_pneumonia) # Create the new pneumonia folder if it does not exist\n",
    "        # Copy the images to the new folders according to their labels\n",
    "        train_list = [f for f in os.listdir(input_path)] # List of all the images\n",
    "        for f in train_list: # Iterate over all the images\n",
    "            file = pydicom.read_file(input_path + f) # Read the image\n",
    "            image = file.pixel_array # Get the image array\n",
    "            id = os.path.splitext(f)[0] # Get the image id\n",
    "            id_label = labels[labels['patientId'] == id]['Target'].values[0] # Get the image label\n",
    "            if id_label == 0: # If the image is normal\n",
    "                cv2.imwrite(output_path_normal + f.replace('.dcm','.jpg'), image) # Save the image in the normal folder\n",
    "            else: # If the image is pneumonia\n",
    "                cv2.imwrite(output_path_pneumonia + f.replace('.dcm','.jpg'), image) # Save the image in the pneumonia folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data objects already exist in directory and load them, if not, create them\n",
    "if not (os.path.exists(\"control_p.pt\") and os.path.exists(\"disease_p.pt\") and os.path.exists(\"control_a.pt\") and os.path.exists(\"disease_a.pt\")):\n",
    "    # Loading the data for the pediatric dataset and resizing the images to 200x200\n",
    "    dataset_pediatric = ImageFolder(root=downloads_path + \"/archive/chest_xray/chest_xray/train\", \n",
    "                                    transform=transforms.Compose([transforms.Resize((200, 200)), transforms.ToTensor()]))\n",
    "\n",
    "    # Loading the data for the adult dataset and resizing the images to 200x200\n",
    "    dataset_adult = ImageFolder(root=downloads_path + \"/rsna-pneumonia-detection-challenge/train\",\n",
    "                                transform=transforms.Compose([transforms.Resize((200, 200)), transforms.ToTensor()]))\n",
    "\n",
    "    # Check numbers of disease and control in each age group to identify the minority class\n",
    "    disease_pediatric = dataset_pediatric.targets.count(1) # Count the number of disease images  \n",
    "    control_pediatric = dataset_pediatric.targets.count(0) # Count the number of control images\n",
    "    disease_adult = dataset_adult.targets.count(1) # Count the number of disease images\n",
    "    control_adult = dataset_adult.targets.count(0) # Count the number of control images\n",
    "    print(f'Pediatric: Control: {control_pediatric}, Disease: {disease_pediatric}\\nAdult: Control: {control_adult}, Disease: {disease_adult}') # Print the numbers\n",
    "\n",
    "    # Set minority class to size 1000 and sample down the majority class to match\n",
    "    minority_size = 1000 # Set the number of control images to 1000\n",
    "    \n",
    "    # Remove control images and targets from the pediatric dataset to sample down to match number of control images of minority class\n",
    "    for i in range(control_pediatric - minority_size): # Iterate over the number of disease images minus the number of control images\n",
    "        dataset_pediatric.imgs.remove(dataset_pediatric.imgs[0]) # Remove the last image\n",
    "        dataset_pediatric.targets.remove(dataset_pediatric.targets[0]) # Remove the last target\n",
    "    \n",
    "    # Remove disease images and targets from the pediatric dataset to sample down to match number of control images of minority class\n",
    "    for i in range(disease_pediatric - minority_size): # Iterate over the number of disease images minus the number of control images\n",
    "        dataset_pediatric.imgs.remove(dataset_pediatric.imgs[-1]) # Remove the last image\n",
    "        dataset_pediatric.targets.remove(dataset_pediatric.targets[-1]) # Remove the last target\n",
    "\n",
    "    # Remove control images and targets from the adult dataset to sample down to match number of control images of minority class\n",
    "    for i in range(control_adult - minority_size): # Iterate over the number of control images minus the number of control images\n",
    "        dataset_adult.imgs.remove(dataset_adult.imgs[0]) # Remove the first image\n",
    "        dataset_adult.targets.remove(dataset_adult.targets[0]) # Remove the first target\n",
    "        \n",
    "    # Remove disease images and targets from the adult dataset to sample down to match number of control images of minority class\n",
    "    for i in range(disease_adult - minority_size): # Iterate over the number of disease images minus the number of control images\n",
    "        dataset_adult.imgs.remove(dataset_adult.imgs[-1]) # Remove the last image\n",
    "        dataset_adult.targets.remove(dataset_adult.targets[-1]) # Remove the last target\n",
    "        \n",
    "    # Check numbers of disease and control in each age group to see if they match\n",
    "    disease_pediatric = dataset_pediatric.targets.count(1) # Count the number of disease images\n",
    "    control_pediatric = dataset_pediatric.targets.count(0) # Count the number of control images\n",
    "    disease_adult = dataset_adult.targets.count(1) # Count the number of disease images\n",
    "    control_adult = dataset_adult.targets.count(0) # Count the number of control images\n",
    "    print(f'Pediatric: Control: {control_pediatric}, Disease: {disease_pediatric}\\nAdult: Control: {control_adult}, Disease: {disease_adult}') # Print the numbers\n",
    "\n",
    "    # Split the pediatric and adult datasets into control and disease yielding 4 datasets for the different classes\n",
    "    control_p, disease_p, control_a, disease_a = [], [], [], [] # Initialize the lists\n",
    "    for i in range(len(dataset_pediatric)): # Iterate over the pediatric dataset\n",
    "        if dataset_pediatric[i][1] == 0: # If the image is control\n",
    "            control_p.append(dataset_pediatric[i]) # Append the image to the control list\n",
    "        else: # If the image is disease\n",
    "            disease_p.append(dataset_pediatric[i]) # Append the image to the disease list\n",
    "    for i in range(len(dataset_adult)): # Iterate over the adult dataset\n",
    "        if dataset_adult[i][1] == 0: # If the image is control\n",
    "            control_a.append(dataset_adult[i]) # Append the image to the control list\n",
    "        else: # If the image is disease\n",
    "            disease_a.append(dataset_adult[i]) # Append the image to the disease list\n",
    "    \n",
    "    # Save the datasets\n",
    "    torch.save(control_p, \"control_p.pt\") # Save the control pediatric dataset\n",
    "    torch.save(disease_p, \"disease_p.pt\") # Save the disease pediatric dataset\n",
    "    torch.save(control_a, \"control_a.pt\") # Save the control adult dataset\n",
    "    torch.save(disease_a, \"disease_a.pt\") # Save the disease adult dataset\n",
    "else:\n",
    "    # Load the datasets\n",
    "    control_p = torch.load(\"control_p.pt\") # Load the control pediatric dataset\n",
    "    disease_p = torch.load(\"disease_p.pt\") # Load the disease pediatric dataset\n",
    "    control_a = torch.load(\"control_a.pt\") # Load the control adult dataset\n",
    "    disease_a = torch.load(\"disease_a.pt\") # Load the disease adult dataset\n",
    "\n",
    "# Add confounder label to the datasets\n",
    "for i in range(len(control_p)): # Iterate over the control pediatric dataset\n",
    "    control_p[i] = control_p[i] + (0,) # Add the confounder label\n",
    "for i in range(len(disease_p)): # Iterate over the disease pediatric dataset\n",
    "    disease_p[i] = disease_p[i] + (0,) # Add the confounder label\n",
    "for i in range(len(control_a)): # Iterate over the control adult dataset\n",
    "    control_a[i] = control_a[i] + (1,) # Add the confounder label\n",
    "for i in range(len(disease_a)): # Iterate over the disease adult dataset\n",
    "    disease_a[i] = disease_a[i] + (1,) # Add the confounder label\n",
    "    \n",
    "# Create the test dataset by sampling 10% of the images from each class\n",
    "testdata_size = 100 # Calculate the number of images to sample\n",
    "sampling_indices = np.random.choice(range(1000), testdata_size, replace=False) # Sample the indices\n",
    "testdata = [] # Initialize the list\n",
    "for i in range(testdata_size): # Iterate over the number of images to sample\n",
    "    testdata.append(control_p[sampling_indices[i]]) # Append the image to the test dataset\n",
    "    testdata.append(disease_p[sampling_indices[i]]) # Append the image to the test dataset\n",
    "    testdata.append(control_a[sampling_indices[i]]) # Append the image to the test dataset\n",
    "    testdata.append(disease_a[sampling_indices[i]]) # Append the image to the test dataset\n",
    "for i in sorted(sampling_indices, reverse=True): # Iterate over the indices in reverse order\n",
    "    del control_p[i] # Delete the image from the control list\n",
    "    del control_a[i] # Delete the image from the control list\n",
    "    del disease_p[i] # Delete the image from the disease list\n",
    "    del disease_a[i] # Delete the image from the disease list\n",
    "    \n",
    "# Create the balanced training dataset by sampling the same number of images from each class\n",
    "length = 500 # Calculate the number of images to sample\n",
    "sampling_indices = np.random.choice(range(900), length, replace=False) #np.random.choice(range(len(control_p)), length, replace=False) # Sample the indices\n",
    "balanced = [] # Initialize the list\n",
    "for i in range(length): # Iterate over the number of images to sample\n",
    "    balanced.append(control_p[sampling_indices[i]]) # Append the image to the balanced dataset\n",
    "    balanced.append(disease_p[sampling_indices[i]]) # Append the image to the balanced dataset\n",
    "    balanced.append(control_a[sampling_indices[i]]) # Append the image to the balanced dataset\n",
    "    balanced.append(disease_a[sampling_indices[i]]) # Append the image to the balanced dataset\n",
    "    \n",
    "# Create confounder datasets for total confounding\n",
    "# Total confounding: Create traindata from half pediatric disease and half adult control or vice versa\n",
    "# 1) Pediatric disease data and adult control data\n",
    "# 2) Pediatric control data and adult disease data\n",
    "\n",
    "length_total = 500 # Length of half the training dataset\n",
    "sampling_indices = np.random.choice(range(900), length_total, replace=False) # Sample the indices\n",
    "total_1 = [] # Initialize the list\n",
    "for i in range(length_total): # Iterate over the number of images to sample\n",
    "    total_1.append(disease_p[sampling_indices[i]]) # Append the image to the total dataset\n",
    "    total_1.append(control_a[sampling_indices[i]]) # Append the image to the total dataset\n",
    "sampling_indices = np.random.choice(range(900), length_total, replace=False) # Sample the indices\n",
    "total_2 = [] # Initialize the list\n",
    "for i in range(length_total): # Iterate over the number of images to sample\n",
    "    total_2.append(disease_a[sampling_indices[i]]) # Append the image to the total dataset\n",
    "    total_2.append(control_p[sampling_indices[i]]) # Append the image to the total dataset\n",
    "    \n",
    "# Generate light confounding datasets in steps of 5 percentage points\n",
    "light_confounded_data = {} # Initialize the dictionary\n",
    "for step in range(1,10):\n",
    "    length = int(500 - 50*step) # Calculate the number of images to sample\n",
    "    sampling_indices_1 = np.random.choice(range(900), length, replace=False) # Sample the indices\n",
    "    sampling_indices_2 = np.random.choice(range(900), 500-length, replace=False) # Sample the indices\n",
    "    light_confounded_data[f'{int(50-5*step)}-{int(5*step)}'] = [] # Initialize the list\n",
    "    for i in range(length):\n",
    "        light_confounded_data[f'{int(50-5*step)}-{int(5*step)}'].append(disease_p[sampling_indices_1[i]])\n",
    "        light_confounded_data[f'{int(50-5*step)}-{int(5*step)}'].append(control_a[sampling_indices_1[i]])\n",
    "    for j in range(500-length):\n",
    "        light_confounded_data[f'{int(50-5*step)}-{int(5*step)}'].append(disease_a[sampling_indices_2[j]])\n",
    "        light_confounded_data[f'{int(50-5*step)}-{int(5*step)}'].append(control_p[sampling_indices_2[j]])\n",
    "    \n",
    "# Split traindata in training and validation (internal testing) data\n",
    "total_1_train, total_1_val = torch.utils.data.random_split(total_1, [800, 200]) # Split the dataset\n",
    "total_2_train, total_2_val = torch.utils.data.random_split(total_2, [800, 200]) # Split the dataset\n",
    "balanced_train, balanced_val = torch.utils.data.random_split(balanced, [1600, 400]) # Split the dataset\n",
    "light_confounded_train_val = {} # Initialize the dictionary\n",
    "for key in light_confounded_data.keys():\n",
    "    light_confounded_train_val[key + '_train'], light_confounded_train_val[key + '_val'] = torch.utils.data.random_split(light_confounded_data[key], [800, 200]) # Split the dataset\n",
    "torch.save(light_confounded_data, \"light_confounded_data.pt\") # Save the light confounded datasets\n",
    "torch.save(light_confounded_train_val, \"light_confounded_train_val.pt\") # Save the light confounded training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 4 images of the first balanced train dataset\n",
    "plt.figure(figsize = (20, 20)) # Set the figure size\n",
    "for i, j in enumerate(range(5, 9)): # Iterate over the first 5 images\n",
    "    image = transforms.ToPILImage()(balanced_train[j][0]) # Convert the image to PIL format\n",
    "    plt.subplot(1, 4, i + 1) # Set the subplot\n",
    "    plt.axis('off')\n",
    "    age = 'Child' if balanced_train[j][2] == 0 else 'Adult'\n",
    "    disease = 'Control' if balanced_train[j][1] == 0 else 'Disease'\n",
    "    plt.title(age + ' - ' + disease) # Set the title\n",
    "    plt.imshow(image) # Show the image\n",
    "plt.savefig('cxr_sample.pdf') # Save the plot\n",
    "plt.show() # Show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replication of results from Garcia Santa Cruz, Husch, Hertel (2022):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valloader, device):\n",
    "    model.eval()\n",
    "    if model.num_classes > 1:\n",
    "        metric = MulticlassAUROC(num_classes=model.num_classes, average='macro', thresholds=None) # Initialize AUROC metric\n",
    "    else: \n",
    "        metric = BinaryAUROC() # Initialize AUROC metric\n",
    "    outputs = [model.validation_step(batch, metric, device) for batch in valloader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(model, optimizer, scheduler, trainloader, valloader, epochs, device, print_results=True):\n",
    "    torch.cuda.empty_cache() # Empty GPU cache\n",
    "    train_history = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            _, loss = model.training_step(batch, device)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if model.name == 'SSN':\n",
    "                model.set_delta(trainloader, device)\n",
    "            del batch\n",
    "            torch.cuda.empty_cache()\n",
    "        model.eval()\n",
    "        result = evaluate(model, valloader, device)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        if print_results:\n",
    "            model.epoch_end(epoch, result)\n",
    "        train_history.append(result)\n",
    "    return train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train_results = {'Baseline': {}, 'SSN_1': {}, 'SSN_2': {}} # Initialize the dictionary\n",
    "test_balanced_results = {'Baseline': {}, 'SSN_1': {}, 'SSN_2': {}} # Initialize the dictionary\n",
    "test_inverse_results = {'Baseline': {}, 'SSN_1': {}, 'SSN_2': {}} # Initialize the dictionary\n",
    "keys = [f'{int(50-5*step)}-{int(5*step)}' for step in range(1,10)] # Initialize the keys\n",
    "for key in keys:\n",
    "    train_results['Baseline'][key] = {} # Initialize the dictionary\n",
    "    train_results['SSN_1'][key] = {} # Initialize the dictionary\n",
    "    train_results['SSN_2'][key] = {} # Initialize the dictionary\n",
    "    test_balanced_results['Baseline'][key] = {} # Initialize the dictionary\n",
    "    test_balanced_results['SSN_1'][key] = {} # Initialize the dictionary\n",
    "    test_balanced_results['SSN_2'][key] = {} # Initialize the dictionary\n",
    "    test_inverse_results['Baseline'][key] = {} # Initialize the dictionary\n",
    "    test_inverse_results['SSN_1'][key] = {} # Initialize the dictionary\n",
    "    test_inverse_results['SSN_2'][key] = {} # Initialize the dictionary\n",
    "    for run in range(10):\n",
    "        trainloader = DataLoader(light_confounded_train_val[key + '_train'], batch_size=50, shuffle=True) # Get the trainloader\n",
    "        valloader = DataLoader(light_confounded_train_val[key + '_val'], batch_size=50, shuffle=True) # Get the valloader\n",
    "        Model_Baseline = Baseline(num_classes=2).to(device)\n",
    "        optimizer_Baseline = optim.Adam(Model_Baseline.parameters(), lr=0.001, weight_decay=0.0001) # Create optimizer\n",
    "        scheduler_Baseline = torch.optim.lr_scheduler.OneCycleLR(optimizer_Baseline, max_lr=0.01, epochs=15, steps_per_epoch=len(trainloader))\n",
    "        Model_SSN_1 = SemiStructuredNet(batch_size=50, cf_dim=1, num_classes=2, num_features=128).to(device)\n",
    "        optimizer_SSN_1 = optim.Adam(Model_SSN_1.parameters(), lr=0.001, weight_decay=0.0001) # Create optimizer\n",
    "        scheduler_SSN_1 = torch.optim.lr_scheduler.OneCycleLR(optimizer_SSN_1, max_lr=0.01, epochs=15, steps_per_epoch=len(trainloader))\n",
    "        Model_SSN_2 = SemiStructuredNet(batch_size=50, cf_dim=2, num_classes=2, num_features=128).to(device)\n",
    "        optimizer_SSN_2 = optim.Adam(Model_SSN_2.parameters(), lr=0.001, weight_decay=0.0001) # Create optimizer\n",
    "        scheduler_SSN_2 = torch.optim.lr_scheduler.OneCycleLR(optimizer_SSN_2, max_lr=0.01, epochs=15, steps_per_epoch=len(trainloader))\n",
    "        print(f'Light confounding: {key}')\n",
    "        # Train the baseline model\n",
    "        print('Baseline')\n",
    "        train_results['Baseline'][key][run] = fit(Model_Baseline, optimizer_Baseline, scheduler_Baseline, trainloader, valloader, epochs = 15, device=device, print_results=False)\n",
    "        # Train the SSN models\n",
    "        print('SSN_1')\n",
    "        train_results['SSN_1'][key][run] = fit(Model_SSN_1, optimizer_SSN_1, scheduler_SSN_1, trainloader, valloader, epochs = 15, device=device, print_results=False)\n",
    "        print('SSN_2')\n",
    "        train_results['SSN_2'][key][run] = fit(Model_SSN_2, optimizer_SSN_2, scheduler_SSN_2, trainloader, valloader, epochs = 15, device=device, print_results=False)\n",
    "        # Test the models\n",
    "        print('Test Balanced')\n",
    "        testloader = DataLoader(testdata, batch_size=50, shuffle=True) # Get the testloader\n",
    "        test_balanced_results['Baseline'][key][run] = evaluate(Model_Baseline, testloader, device)['val_auc']\n",
    "        test_balanced_results['SSN_1'][key][run] = evaluate(Model_SSN_1, testloader, device)['val_auc']\n",
    "        test_balanced_results['SSN_2'][key][run] = evaluate(Model_SSN_2, testloader, device)['val_auc']\n",
    "        print('Test Inverse')\n",
    "        testloader = DataLoader(light_confounded_train_val[key.split('-')[1] + '-' + key.split('-')[0] + '_val'], batch_size=50, shuffle=True) # Get the testloader\n",
    "        test_inverse_results['Baseline'][key][run] = evaluate(Model_Baseline, testloader, device)['val_auc']\n",
    "        test_inverse_results['SSN_1'][key][run] = evaluate(Model_SSN_1, testloader, device)['val_auc']\n",
    "        test_inverse_results['SSN_2'][key][run] = evaluate(Model_SSN_2, testloader, device)['val_auc']\n",
    "        # Save fitted models\n",
    "        torch.save(Model_Baseline.state_dict(), f\"fitted_models/Baseline_{key}_{run}.pt\")\n",
    "        torch.save(Model_SSN_1.state_dict(), f\"fitted_models/SSN_1_{key}_{run}.pt\")\n",
    "        torch.save(Model_SSN_2.state_dict(), f\"fitted_models/SSN_2_{key}_{run}.pt\")\n",
    "torch.save(train_results, \"train_results.pt\") # Save the training results\n",
    "torch.save(test_balanced_results, \"test_balanced_results.pt\") # Save the test results\n",
    "torch.save(test_inverse_results, \"test_inverse_results.pt\") # Save the test results\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment\n",
    "os.system('python run_experiment.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = torch.load(\"train_results.pt\") # Load the training results\n",
    "test_balanced_results = torch.load(\"test_balanced_results.pt\") # Load the test results\n",
    "test_inverse_results = torch.load(\"test_inverse_results.pt\") # Load the test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings for plotting\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Make boxplots of results for each confounding level\n",
    "fig, ax = plt.subplots(3, 1, figsize=(8, 15)) # Initialize the figure\n",
    "plot_train_results = pd.DataFrame(columns=['Model', 'Confounding Degree', 'AUC']) # Initialize the dataframe\n",
    "Models = ['Baseline', 'SSN_1', 'SSN_2'] # Initialize the models vector\n",
    "Cf_degrees = ['25-25', '30-20', '20-30', '35-15', '15-35', '40-10', '10-40', '45-5', '5-45'] # Initialize the confounding degrees vector\n",
    "for model in Models:\n",
    "    for cf_degree in Cf_degrees:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            aucs.append(train_results[model][cf_degree][i][-1]['val_auc'])\n",
    "        plot_train_results = plot_train_results.append(pd.DataFrame({'Model': [model] * 10, 'Confounding Degree': [cf_degree] * 10, 'AUC': aucs}), ignore_index=True)\n",
    "sns.boxplot(ax=ax[0], x='Confounding Degree', y='AUC', hue='Model', data=plot_train_results, palette='tab10')\n",
    "ax[0].set_title('AUC on (Internal) Validation Set')\n",
    "plot_balanced_results = pd.DataFrame(columns=['Model', 'Confounding Degree', 'AUC']) # Initialize the dataframe\n",
    "for model in Models:\n",
    "    for cf_degree in Cf_degrees:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            aucs.append(test_balanced_results[model][cf_degree][i])\n",
    "        plot_balanced_results = plot_balanced_results.append(pd.DataFrame({'Model': [model] * 10, 'Confounding Degree': [cf_degree] * 10, 'AUC': aucs}), ignore_index=True)\n",
    "sns.boxplot(ax=ax[1], x='Confounding Degree', y='AUC', hue='Model', data=plot_balanced_results, palette='tab10')\n",
    "ax[1].set_title('AUC on (External) Balanced Test Set')\n",
    "plot_inverse_results = pd.DataFrame(columns=['Model', 'Confounding Degree', 'AUC']) # Initialize the dataframe\n",
    "for model in Models:\n",
    "    for cf_degree in Cf_degrees:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            aucs.append(test_inverse_results[model][cf_degree][i])\n",
    "        plot_inverse_results = plot_inverse_results.append(pd.DataFrame({'Model': [model] * 10, 'Confounding Degree': [cf_degree] * 10, 'AUC': aucs}), ignore_index=True)\n",
    "sns.boxplot(ax=ax[2], x='Confounding Degree', y='AUC', hue='Model', data=plot_inverse_results, palette='tab10')\n",
    "ax[2].set_title('AUC on \"Inverse\" Validation Set')\n",
    "fig.tight_layout(pad=3.0)\n",
    "plt.savefig('results_AUC_1.pdf') # Save the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make boxplots for the difference between the AUC on the balanced test set and the AUC on the inverse validation set\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5)) # Initialize the figure\n",
    "plot_results = pd.DataFrame(columns=['Model', 'Confounding Degree', 'AUC']) # Initialize the dataframe\n",
    "for model in Models:\n",
    "    for cf_degree in Cf_degrees:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            aucs.append(np.abs(test_balanced_results[model][cf_degree][i] - test_inverse_results[model][cf_degree][i]))\n",
    "        plot_results = plot_results.append(pd.DataFrame({'Model': [model] * 10, 'Confounding Degree': [cf_degree] * 10, 'AUC': aucs}), ignore_index=True)\n",
    "sns.boxplot(ax=ax[1], x='Confounding Degree', y='AUC', hue='Model', data=plot_results, palette='tab10')\n",
    "ax[1].set_title('Balanced Test Set vs. \"Inverse\" Validation Set')\n",
    "plot_results = pd.DataFrame(columns=['Model', 'Confounding Degree', 'AUC']) # Initialize the dataframe\n",
    "for model in Models:\n",
    "    for cf_degree in Cf_degrees:\n",
    "        aucs = []\n",
    "        for i in range(10):\n",
    "            aucs.append(np.abs(test_balanced_results[model][cf_degree][i] - train_results[model][cf_degree][i][-1]['val_auc']))\n",
    "        plot_results = plot_results.append(pd.DataFrame({'Model': [model] * 10, 'Confounding Degree': [cf_degree] * 10, 'AUC': aucs}), ignore_index=True)\n",
    "sns.boxplot(ax=ax[0], x='Confounding Degree', y='AUC', hue='Model', data=plot_results, palette='tab10')\n",
    "ax[0].set_title('Balanced Test Set vs. Validation Set')\n",
    "plt.savefig('results_AUC_2.pdf') # Save the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appendix (old versions of plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare AUCs for plotting\n",
    "internal_aucs = {'Baseline': [], 'SSN_1': [], 'SSN_2': []}\n",
    "external_balanced_aucs = {'Baseline': [], 'SSN_1': [], 'SSN_2': []}\n",
    "external_inverse_aucs = {'Baseline': [], 'SSN_1': [], 'SSN_2': []}\n",
    "for step in range(1,10):\n",
    "    A, B, C, D, E, F, G, H, I = [], [], [], [], [], [], [], [], []\n",
    "    for run in range(10):\n",
    "        A += [train_results['Baseline'][f'{int(50-5*step)}-{int(5*step)}'][run][-1]['val_auc']]\n",
    "        B += [train_results['SSN_1'][f'{int(50-5*step)}-{int(5*step)}'][run][-1]['val_auc']]\n",
    "        C += [train_results['SSN_2'][f'{int(50-5*step)}-{int(5*step)}'][run][-1]['val_auc']]\n",
    "        D += [test_balanced_results['Baseline'][f'{int(50-5*step)}-{int(5*step)}'][run]]\n",
    "        E += [test_balanced_results['SSN_1'][f'{int(50-5*step)}-{int(5*step)}'][run]]\n",
    "        F += [test_balanced_results['SSN_2'][f'{int(50-5*step)}-{int(5*step)}'][run]]\n",
    "        G += [test_inverse_results['Baseline'][f'{int(50-5*step)}-{int(5*step)}'][run]]\n",
    "        H += [test_inverse_results['SSN_1'][f'{int(50-5*step)}-{int(5*step)}'][run]]\n",
    "        I += [test_inverse_results['SSN_2'][f'{int(50-5*step)}-{int(5*step)}'][run]]\n",
    "    internal_aucs['Baseline'].append(np.mean(A))\n",
    "    internal_aucs['SSN_1'].append(np.mean(B))\n",
    "    internal_aucs['SSN_2'].append(np.mean(C))\n",
    "    external_balanced_aucs['Baseline'].append(np.mean(D))\n",
    "    external_balanced_aucs['SSN_1'].append(np.mean(E))\n",
    "    external_balanced_aucs['SSN_2'].append(np.mean(F))\n",
    "    external_inverse_aucs['Baseline'].append(np.mean(G))\n",
    "    external_inverse_aucs['SSN_1'].append(np.mean(H))\n",
    "    external_inverse_aucs['SSN_2'].append(np.mean(I))\n",
    "internal_auc_Baseline_1 = internal_aucs['Baseline'][:4]\n",
    "internal_auc_Baseline_2 = internal_aucs['Baseline'][5:]\n",
    "external_balanced_auc_Baseline_1 = external_balanced_aucs['Baseline'][:4]\n",
    "external_balanced_auc_Baseline_2 = external_balanced_aucs['Baseline'][5:]\n",
    "external_inverse_auc_Baseline_1 = external_inverse_aucs['Baseline'][:4]\n",
    "external_inverse_auc_Baseline_2 = external_inverse_aucs['Baseline'][5:]\n",
    "internal_auc_SSN_1_1 = internal_aucs['SSN_1'][:4]\n",
    "internal_auc_SSN_1_2 = internal_aucs['SSN_1'][5:]\n",
    "external_balanced_auc_SSN_1_1 = external_balanced_aucs['SSN_1'][:4]\n",
    "external_balanced_auc_SSN_1_2 = external_balanced_aucs['SSN_1'][5:]\n",
    "external_inverse_auc_SSN_1_1 = external_inverse_aucs['SSN_1'][:4]\n",
    "external_inverse_auc_SSN_1_2 = external_inverse_aucs['SSN_1'][5:]\n",
    "internal_auc_SSN_2_1 = internal_aucs['SSN_2'][:4]\n",
    "internal_auc_SSN_2_2 = internal_aucs['SSN_2'][5:]\n",
    "external_balanced_auc_SSN_2_1 = external_balanced_aucs['SSN_2'][:4]\n",
    "external_balanced_auc_SSN_2_2 = external_balanced_aucs['SSN_2'][5:]\n",
    "external_inverse_auc_SSN_2_1 = external_inverse_aucs['SSN_2'][:4]\n",
    "external_inverse_auc_SSN_2_2 = external_inverse_aucs['SSN_2'][5:]\n",
    "internal_auc_Baseline_1.reverse()\n",
    "internal_auc_SSN_1_1.reverse()\n",
    "internal_auc_SSN_2_1.reverse()\n",
    "external_balanced_auc_Baseline_1.reverse()\n",
    "external_balanced_auc_SSN_1_1.reverse()\n",
    "external_balanced_auc_SSN_2_1.reverse()\n",
    "external_inverse_auc_Baseline_1.reverse()\n",
    "external_inverse_auc_SSN_1_1.reverse()\n",
    "external_inverse_auc_SSN_2_1.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iB, iSSN1, iSSN2 = [], [], []\n",
    "ebB, ebSSN1, ebSSN2 = [], [], []\n",
    "eiB, eiSSN1, eiSSN2 = [], [], []\n",
    "for run in range(10):\n",
    "    iB += [train_results['Baseline']['25-25'][run][-1]['val_auc']]\n",
    "    iSSN1 += [train_results['SSN_1']['25-25'][run][-1]['val_auc']]\n",
    "    iSSN2 += [train_results['SSN_2']['25-25'][run][-1]['val_auc']]\n",
    "    ebB += [test_balanced_results['Baseline']['25-25'][run]]\n",
    "    ebSSN1 += [test_balanced_results['SSN_1']['25-25'][run]]\n",
    "    ebSSN2 += [test_balanced_results['SSN_2']['25-25'][run]]\n",
    "    eiB += [test_inverse_results['Baseline']['25-25'][run]]\n",
    "    eiSSN1 += [test_inverse_results['SSN_1']['25-25'][run]]\n",
    "    eiSSN2 += [test_inverse_results['SSN_2']['25-25'][run]]\n",
    "internal_auc_Baseline, internal_auc_SSN_1, internal_auc_SSN_2 = [np.mean(iB)], [np.mean(iSSN1)], [np.mean(iSSN2)]\n",
    "external_balanced_auc_Baseline, external_balanced_auc_SSN_1, external_balanced_auc_SSN_2 = [np.mean(ebB)], [np.mean(ebSSN1)], [np.mean(ebSSN2)]\n",
    "external_inverse_auc_Baseline, external_inverse_auc_SSN_1, external_inverse_auc_SSN_2 = [np.mean(eiB)], [np.mean(eiSSN1)], [np.mean(eiSSN2)]\n",
    "for i in range(4):\n",
    "    internal_auc_Baseline.append(internal_auc_Baseline_1[i])\n",
    "    internal_auc_Baseline.append(internal_auc_Baseline_2[i])\n",
    "    internal_auc_SSN_1.append(internal_auc_SSN_1_1[i])\n",
    "    internal_auc_SSN_1.append(internal_auc_SSN_1_2[i])\n",
    "    internal_auc_SSN_2.append(internal_auc_SSN_2_1[i])\n",
    "    internal_auc_SSN_2.append(internal_auc_SSN_2_2[i])\n",
    "    external_balanced_auc_Baseline.append(external_balanced_auc_Baseline_1[i])\n",
    "    external_balanced_auc_Baseline.append(external_balanced_auc_Baseline_2[i])\n",
    "    external_balanced_auc_SSN_1.append(external_balanced_auc_SSN_1_1[i])\n",
    "    external_balanced_auc_SSN_1.append(external_balanced_auc_SSN_1_2[i])\n",
    "    external_balanced_auc_SSN_2.append(external_balanced_auc_SSN_2_1[i])\n",
    "    external_balanced_auc_SSN_2.append(external_balanced_auc_SSN_2_2[i])\n",
    "    external_inverse_auc_Baseline.append(external_inverse_auc_Baseline_1[i])\n",
    "    external_inverse_auc_Baseline.append(external_inverse_auc_Baseline_2[i])\n",
    "    external_inverse_auc_SSN_1.append(external_inverse_auc_SSN_1_1[i])\n",
    "    external_inverse_auc_SSN_1.append(external_inverse_auc_SSN_1_2[i])\n",
    "    external_inverse_auc_SSN_2.append(external_inverse_auc_SSN_2_1[i])\n",
    "    external_inverse_auc_SSN_2.append(external_inverse_auc_SSN_2_2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the AUCs\n",
    "fig, ax = plt.subplots(1,3, figsize=(20,5))\n",
    "ax[0].plot(range(1,10), internal_auc_Baseline, '-b', label='Baseline')\n",
    "ax[0].plot(range(1,10), internal_auc_SSN_1, '-r', label='SSN (no constant)')\n",
    "ax[0].plot(range(1,10), internal_auc_SSN_2, '-g', label='SSN (with constant)')\n",
    "ax[0].set_title('AUC on (Internal) Validation Set')\n",
    "ax[1].plot(range(1,10), external_balanced_auc_Baseline, '-b', label='Baseline')\n",
    "ax[1].plot(range(1,10), external_balanced_auc_SSN_1, '-r', label='SSN (no constant)')\n",
    "ax[1].plot(range(1,10), external_balanced_auc_SSN_2, '-g', label='SSN (with constant)')\n",
    "ax[1].set_title('AUC on (External) Balanced Test Set')\n",
    "ax[2].plot(range(1,10), external_inverse_auc_Baseline, '-b', label='Baseline')\n",
    "ax[2].plot(range(1,10), external_inverse_auc_SSN_1, '-r', label='SSN (no constant)')\n",
    "ax[2].plot(range(1,10), external_inverse_auc_SSN_2, '-g', label='SSN (with constant)')\n",
    "ax[2].set_title('AUC on \"inverted\" Validation Set')\n",
    "for i in range(3): \n",
    "    ax[i].set_xticks(range(1,10), ['25-25', '30-20', '20-30', '35-15', '15-35', '40-10', '10-40', '45-5', '5-45'])\n",
    "    ax[i].set_ylim(0.5, 1)\n",
    "    ax[i].set_xlabel('Degree of confounding')\n",
    "    ax[i].set_ylabel('AUC')\n",
    "    ax[i].legend()\n",
    "plt.savefig('AUCs.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUC differences\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "ax[0].plot(range(1,10), np.abs(np.array(internal_auc_Baseline) - np.array(external_balanced_auc_Baseline)), '-b', label='Baseline')\n",
    "ax[0].plot(range(1,10), np.abs(np.array(internal_auc_SSN_1) - np.array(external_balanced_auc_SSN_1)), '-r', label='SSN (no constant)')\n",
    "ax[0].plot(range(1,10), np.abs(np.array(internal_auc_SSN_2) - np.array(external_balanced_auc_SSN_2)), '-g', label='SSN (with constant)')\n",
    "ax[1].plot(range(1,10), np.abs(np.array(internal_auc_Baseline) - np.array(external_inverse_auc_Baseline)), '-b', label='Baseline')\n",
    "ax[1].plot(range(1,10), np.abs(np.array(internal_auc_SSN_1) - np.array(external_inverse_auc_SSN_1)), '-r', label='SSN (no constant)')\n",
    "ax[1].plot(range(1,10), np.abs(np.array(internal_auc_SSN_2) - np.array(external_inverse_auc_SSN_2)), '-g', label='SSN (with constant)')\n",
    "for i in range(2):\n",
    "    ax[i].set_xticks(range(1,10), ['25-25', '30-20', '20-30', '35-15', '15-35', '40-10', '10-40', '45-5', '5-45'])\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel('Degree of confounding')\n",
    "    ax[i].set_ylabel('Absolute difference')\n",
    "    ax[i].set_title('Absolute difference between AUC on internal validation set and external (balanced) test set')\n",
    "plt.savefig('AUC_difference_balanced.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
