{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import shap \n",
    "import os\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pickle # for saving to file\n",
    "import scipy.stats as stats # for 95% CI\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib as mlp\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from torchview import draw_graph\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from torchmetrics.regression import SpearmanCorrCoef\n",
    "from torch.utils.data import DataLoader\n",
    "import seaborn as sns\n",
    "\n",
    "from models_SSN import Baseline, SemiStructuredNet, IRM\n",
    "from datasets_SSN import create_dataloader, prepare_data, plot_colored_mnist, make_environment, ColoredMNIST\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Set default device\n",
    "#torch.set_default_device(\"cpu\")\n",
    "# Use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:1\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "# https://medium.com/analytics-vidhya/multiclass-image-classification-with-pytorch-af7578e10ee6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for model fitting and evaluation as well as plotting\n",
    "def evaluate(model, valloader, model_type, device, cf_dim=2):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch, model_type, cf_dim, device) for batch in valloader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(model, optimizer, trainloader, valloader, testloader, model_type, batch_size=100, cf_dim=2, num_features=32, epochs=50, device=torch.device(\"cpu\")):\n",
    "    train_history = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in trainloader:\n",
    "            if model_type == 'SSN':\n",
    "                cfs = batch['colors'].to(device)\n",
    "                model.set_confounders(cfs, cf_dim, device)\n",
    "            optimizer.zero_grad()\n",
    "            pred, loss = model.training_step(batch, model_type, cf_dim, device)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if model_type == 'SSN':\n",
    "            model.set_delta(trainloader, cf_dim, num_features, device)\n",
    "        # Internal Validation\n",
    "        model.eval()\n",
    "        result = evaluate(model, valloader, model_type, device, cf_dim)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        train_history.append(result)\n",
    "        # External Validation\n",
    "        test = evaluate(model, testloader, model_type, device, cf_dim)\n",
    "        train_history[-1]['test_acc'] = test['val_acc']\n",
    "        # Early stopping\n",
    "        if result['val_loss'] < best_val_loss:\n",
    "            torch.save(model.state_dict(), f'fitted_models/ColoredMNIST/best-model-{model_type}-{batch_size}.pth')\n",
    "            print(f\"Model saved: {result['val_loss']}\")\n",
    "            best_val_loss = result['val_loss']\n",
    "            patience = 5\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience == 0:\n",
    "                model.load_state_dict(torch.load(f'fitted_models/ColoredMNIST/best-model-{model_type}-{batch_size}.pth'))\n",
    "                break\n",
    "    return train_history\n",
    "\n",
    "def plot_train_history(history, model, figsize):\n",
    "    acc_val = [x['val_acc'] for x in history] \n",
    "    acc_test = [x['test_acc'] for x in history]\n",
    "    train_losses = [x.get('train_loss') for x in history]\n",
    "    val_losses = [x['val_loss'] for x in history]\n",
    "    mlp.style.use('default')\n",
    "    if model == 'Baseline':\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        gs = gridspec.GridSpec(nrows=2, ncols=2)\n",
    "        ax0 = fig.add_subplot(gs[0, 0]) \n",
    "        ax0.plot(acc_val, '-r')\n",
    "        ax0.set_ylabel('Accuracy')\n",
    "        ax0.set_title('Validation and Test Accuracy')\n",
    "        ax0.set_ylim(min(acc_val)*.99, max(acc_val)*1.01)\n",
    "        ax0.spines['bottom'].set_visible(False)\n",
    "        ax0.xaxis.tick_top()\n",
    "        ax0.xaxis.set_major_locator(plt.NullLocator())\n",
    "        ax0.tick_params(labeltop=False)\n",
    "        ax1 = fig.add_subplot(gs[1, 0])\n",
    "        ax1.plot(acc_test, '-g')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.set_ylim(min(acc_test)*.99, max(acc_test)*1.01)\n",
    "        ax1.spines['top'].set_visible(False)\n",
    "        ax1.xaxis.tick_bottom()\n",
    "        custom_lines = [Line2D([0], [0], color='r', lw=4),\n",
    "                        Line2D([0], [0], color='g', lw=4)]\n",
    "        ax0.legend(custom_lines, ['Validation', 'Test'], frameon=False)\n",
    "        ax2 = fig.add_subplot(gs[:, 1])\n",
    "        ax2.plot(train_losses, '-b')\n",
    "        ax2.plot(val_losses, '-r')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.yaxis.set_label_position(\"right\")\n",
    "        ax2.yaxis.tick_right()\n",
    "        ax2.set_title('Training and Validation Loss')\n",
    "        custom_lines = [Line2D([0], [0], color='b', lw=4),\n",
    "                        Line2D([0], [0], color='r', lw=4)]\n",
    "        ax2.legend(custom_lines, ['Training', 'Validation'], frameon=False)\n",
    "        d = .015\n",
    "        kwargs = dict(transform=ax0.transAxes, color='k', clip_on=False)\n",
    "        ax0.plot((-d, +d), (-d, +d), **kwargs)\n",
    "        ax0.plot((1 - d, 1 + d), (-d, +d), **kwargs)\n",
    "        kwargs.update(transform=ax1.transAxes)\n",
    "        ax1.plot((-d, +d), (1 - d, 1 + d), **kwargs)\n",
    "        ax1.plot((1 - d, 1 + d), (1 - d, 1 + d), **kwargs)\n",
    "    else:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=figsize)\n",
    "        axs[0].plot(acc_val, '-r')\n",
    "        axs[0].plot(acc_test, '-g')\n",
    "        axs[0].set_xlabel('Epoch')\n",
    "        axs[0].set_ylabel('Accuracy')\n",
    "        custom_lines = [Line2D([0], [0], color='r', lw=4),\n",
    "                        Line2D([0], [0], color='g', lw=4)]\n",
    "        axs[0].legend(custom_lines, ['Validation', 'Test'], frameon=False)\n",
    "        axs[0].set_title('Validation and Test Accuracy')\n",
    "        axs[1].plot(train_losses, '-b')\n",
    "        axs[1].plot(val_losses, '-r')\n",
    "        axs[1].set_xlabel('Epoch')\n",
    "        axs[1].set_ylabel('Loss')\n",
    "        axs[1].yaxis.set_label_position(\"right\")\n",
    "        axs[1].yaxis.tick_right()\n",
    "        axs[1].set_title('Training and Validation Loss')\n",
    "        custom_lines = [Line2D([0], [0], color='b', lw=4),\n",
    "                        Line2D([0], [0], color='r', lw=4)]\n",
    "        axs[1].legend(custom_lines, ['Training', 'Validation'], frameon=False)\n",
    "    plt.savefig(f'plots/ColoredMNIST/training_history_{model}.pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "preparation = prepare_data()\n",
    "dataloader = create_dataloader(**preparation, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plots for ColouredMNIST samples\n",
    "plot_colored_mnist(next(iter(dataloader['train'])), 0.2)\n",
    "plot_colored_mnist(next(iter(dataloader['test'])), 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation between true labels, flipped labels and colours for one batch of data\n",
    "# Training set\n",
    "labels = torch.tensor([])\n",
    "true_labels = torch.tensor([])\n",
    "colors = torch.tensor([])\n",
    "for batch in dataloader['train']:\n",
    "    labels = torch.cat((labels, batch['labels']), 0)\n",
    "    true_labels = torch.cat((true_labels, batch['true_labels']), 0)\n",
    "    colors = torch.cat((colors, batch['colors']), 0)\n",
    "corrdata = pd.DataFrame({'Flipped Labels': labels.T.int().numpy().squeeze(), 'True Labels': true_labels.T.int().numpy().squeeze(), 'Colours': colors.T.int().numpy().squeeze()})\n",
    "# Test set\n",
    "labels_test = torch.tensor([])\n",
    "true_labels_test = torch.tensor([])\n",
    "colors_test = torch.tensor([])\n",
    "for batch in dataloader['test']:\n",
    "    labels_test = torch.cat((labels_test, batch['labels']), 0)\n",
    "    true_labels_test = torch.cat((true_labels_test, batch['true_labels']), 0)\n",
    "    colors_test = torch.cat((colors_test, batch['colors']), 0)\n",
    "corrdata_test = pd.DataFrame({'Flipped Labels': labels_test.T.int().numpy().squeeze(), 'True Labels': true_labels_test.T.int().numpy().squeeze(), 'Colors': colors_test.T.int().numpy().squeeze()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation heatmaps\n",
    "mask = np.triu(np.ones_like(corrdata.corr(), dtype=np.bool_))\n",
    "training_corr = sns.heatmap(corrdata.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "fig = training_corr.get_figure()\n",
    "fig.savefig('plots/ColoredMNIST/training_corr.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation heatmaps\n",
    "mask = np.triu(np.ones_like(corrdata_test.corr(), dtype=np.bool_))\n",
    "testing_corr = sns.heatmap(corrdata_test.corr(), mask=mask, vmin=-1,vmax=1, annot=True, cmap='BrBG')\n",
    "fig = testing_corr.get_figure()\n",
    "fig.savefig('plots/ColoredMNIST/testing_corr.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate Baseline model\n",
    "device = torch.device(\"cpu\")\n",
    "BaselineModel = Baseline().to(device)\n",
    "BaselineOptimizer = optim.Adam(BaselineModel.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "print('Training Baseline model...')\n",
    "Baseline_train_history = fit(BaselineModel, BaselineOptimizer, \n",
    "                             dataloader['train'], dataloader['val'], dataloader['test'], \n",
    "                             batch_size=100, model_type='Baseline', epochs=50)\n",
    "print('\\nEvaluating Baseline model...')\n",
    "Baseline_test = evaluate(BaselineModel, dataloader['test'], model_type='Baseline', device=device)\n",
    "print(f'Testing Loss: {Baseline_test[\"val_loss\"]:.4f}, Testing Accuracy: {Baseline_test[\"val_acc\"]:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model architecture\n",
    "model_graph = draw_graph(BaselineModel, input_size=(100, 2*14*14), device ='meta', graph_dir='TB', hide_module_functions=True, \n",
    "                         save_graph=True, directory='plots/ColoredMNIST', filename='Baseline_model_graph.pdf')\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results of training Baseline model\n",
    "plot_train_history(Baseline_train_history, model='Baseline', figsize=(8, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate Semi-Structured model\n",
    "device = torch.device(\"cpu\")\n",
    "SemiStructuredModel = SemiStructuredNet(batch_size=100, cf_dim=2, num_features=32).to(device)\n",
    "SemiStructuredModelOptimizer = optim.Adam(SemiStructuredModel.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "print('Training Semi-Structured model...')\n",
    "SemiStructredModel_train_history = fit(SemiStructuredModel, SemiStructuredModelOptimizer, \n",
    "                                       dataloader['train'], dataloader['val'], dataloader['test'], \n",
    "                                       batch_size=100, model_type='SSN', cf_dim=2, num_features=32, epochs=50, device=device)\n",
    "print('\\nEvaluating Semi-Structured model...')\n",
    "SemiStructured_test = evaluate(SemiStructuredModel, dataloader['test'], model_type='SSN', device=device)\n",
    "print(f'Testing Loss: {SemiStructured_test[\"val_loss\"]:.4f}, Testing Accuracy: {SemiStructured_test[\"val_acc\"]:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results of training Semi-Structured model\n",
    "plot_train_history(SemiStructredModel_train_history, model='SSN', figsize=(8, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get IRM results as benchmark\n",
    "os.system(f'python IRM.py --hidden_dim=390 --l2_regularizer_weight=0.00110794568 --lr=0.0004898536566546834 --penalty_anneal_iters=190 --penalty_weight=91257.18613115903 --steps=501')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate model performance for different batch sizes\n",
    "device = torch.device(\"cpu\")\n",
    "batch_sizes = [50, 100, 1000, 10000]\n",
    "train_history = {} # Dictionary for storing training history for different batch sizes\n",
    "test_history = {} # Dictionary for storing test results for different batch sizes\n",
    "for run in range(10):\n",
    "    train_history[run], test_history[run] = {}, {} # Initialize dictionaries for current run\n",
    "    train_history[run]['Baseline'], test_history[run]['Baseline'] = [], [] # Initialize lists for storing training history and test results for Baseline model\n",
    "    train_history[run]['SSN'], test_history[run]['SSN'] = [], [] # Initialize lists for storing training history and test results for Semi-Structured model\n",
    "    print(f'Run {run+1}...')\n",
    "    preparation = prepare_data() # Prepare data for given batch size\n",
    "    for batch in batch_sizes:\n",
    "        dataloader = create_dataloader(**preparation, batch_size=batch) # Create dataloaders for given batch size\n",
    "        # Create and train models\n",
    "        BaselineModel = Baseline().to(device)\n",
    "        BaselineOptimizer = optim.Adam(BaselineModel.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "        print(f'Training Baseline model with batch size {batch}...')\n",
    "        train_history[run]['Baseline'].append(fit(BaselineModel, BaselineOptimizer, \n",
    "                                                  dataloader['train'], dataloader['val'], dataloader['test'], \n",
    "                                                  batch_size=batch, model_type='Baseline', epochs=50, device=device))\n",
    "        test_history[run]['Baseline'].append(evaluate(BaselineModel, dataloader['test'], model_type='Baseline', device=device))\n",
    "        SemiStructuredModel = SemiStructuredNet(batch_size=batch, cf_dim=2, num_features=32).to(device)\n",
    "        SemiStructuredModelOptimizer = optim.Adam(SemiStructuredModel.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "        print(f'Training Semi-Structured model with batch size {batch}...')\n",
    "        train_history[run]['SSN'].append(fit(SemiStructuredModel, SemiStructuredModelOptimizer, \n",
    "                                             dataloader['train'], dataloader['val'], dataloader['test'], \n",
    "                                             batch_size=batch, model_type='SSN', cf_dim=2, num_features=32, epochs=50, device=device))\n",
    "        test_history[run]['SSN'].append(evaluate(SemiStructuredModel, dataloader['test'], model_type='SSN', device=device))\n",
    "print('Done!')\n",
    "\n",
    "# Save results to file\n",
    "with open('results/ColoredMNIST/train_history.pkl', 'wb') as f:\n",
    "    pickle.dump(train_history, f)\n",
    "    print('Training history saved to file.')\n",
    "with open('results/ColoredMNIST/test_history.pkl', 'wb') as f:\n",
    "    pickle.dump(test_history, f)\n",
    "    print('Test history saved to file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from file\n",
    "with open('results/ColoredMNIST/train_history.pkl', 'rb') as f:\n",
    "    train_history = pickle.load(f)\n",
    "with open('results/ColoredMNIST/test_history.pkl', 'rb') as f:\n",
    "    test_history = pickle.load(f)\n",
    "batch_sizes = [50, 100, 1000, 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results of training for different batch sizes in a table\n",
    "batch_results = pd.DataFrame(columns=['Model', 'Batch Size', 'Final Train Loss', 'Final Train Accuracy', 'Test Loss', 'Test Accuracy'])\n",
    "for model in ['Baseline', 'SSN']:\n",
    "    for i, batch in enumerate(batch_sizes):\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_accs = []\n",
    "        test_accs = []\n",
    "        for run in range(10):\n",
    "            train_losses.append(train_history[run][model][i][-6]['train_loss'])\n",
    "            test_losses.append(test_history[run][model][i]['val_loss'])\n",
    "            train_accs.append(train_history[run][model][i][-6]['val_acc'])\n",
    "            test_accs.append(test_history[run][model][i]['val_acc'])\n",
    "        # Calculate mean and 95% confidence interval for train and test losses and accuracies\n",
    "        train_loss_mean = np.mean(train_losses)\n",
    "        train_loss_ci = stats.t.interval(0.95, len(train_losses)-1, loc=train_loss_mean, scale=stats.sem(train_losses))\n",
    "        test_loss_mean = np.mean(test_losses)\n",
    "        test_loss_ci = stats.t.interval(0.95, len(test_losses)-1, loc=test_loss_mean, scale=stats.sem(test_losses))\n",
    "        train_acc_mean = np.mean(train_accs)\n",
    "        train_acc_ci = stats.t.interval(0.95, len(train_accs)-1, loc=train_acc_mean, scale=stats.sem(train_accs))\n",
    "        test_acc_mean = np.mean(test_accs)\n",
    "        test_acc_ci = stats.t.interval(0.95, len(test_accs)-1, loc=test_acc_mean, scale=stats.sem(test_accs))\n",
    "        # Add results to table\n",
    "        batch_results = batch_results.append({'Model': model, 'Batch Size': batch, \n",
    "                                              'Final Train Loss':  f'{train_loss_mean:.4f}' + u\"\\u00B1\" + f'{train_loss_ci[1]-train_loss_mean:.2f}', \n",
    "                                              'Final Train Accuracy': f'{train_acc_mean:.2%}' + u\"\\u00B1\" + f'{train_acc_ci[1]-train_acc_mean:.2%}', \n",
    "                                              'Test Loss': f'{test_loss_mean:.4f}' + u\"\\u00B1\" + f'{test_loss_ci[1]-test_loss_mean:.2f}', \n",
    "                                              'Test Accuracy': f'{test_acc_mean:.2%}' + u\"\\u00B1\" + f'{test_acc_ci[1]-test_acc_mean:.2%}'}, ignore_index=True)\n",
    "batch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact orthogonalization with full batch size\n",
    "data = prepare_data()\n",
    "trainset = data['trainset']\n",
    "testset = data['testset']\n",
    "mnist_train = (trainset.data, trainset.targets) # (images, labels)\n",
    "mnist_test = (testset.data, testset.targets) # (images, labels)\n",
    "train_val_split = 40000 # Split the training set into training and validation set\n",
    "envs = [\n",
    "    make_environment(mnist_train[0], mnist_train[1], 0.1, 2, True), # Environment for training and validation with 20% of the colors flipped\n",
    "    make_environment(mnist_test[0], mnist_test[1], 0.9, 2, True) # Environment for testing with 90% of the colors flipped\n",
    "]\n",
    "traindata = ColoredMNIST(envs[0]['images'][:train_val_split], \n",
    "                        envs[0]['labels'][:train_val_split], \n",
    "                        envs[0]['colors'][:train_val_split],\n",
    "                        envs[0]['true_labels'][:train_val_split]) # Create ColoredMNIST dataset training set\n",
    "valdata = ColoredMNIST(envs[0]['images'][train_val_split:], \n",
    "                    envs[0]['labels'][train_val_split:], \n",
    "                    envs[0]['colors'][train_val_split:],\n",
    "                    envs[0]['true_labels'][train_val_split:]) # Create ColoredMNIST dataset validation set\n",
    "testdata = ColoredMNIST(envs[1]['images'], \n",
    "                        envs[1]['labels'], \n",
    "                        envs[1]['colors'],\n",
    "                        envs[1]['true_labels']) # Create ColoredMNIST dataset test set\n",
    "trainloader = DataLoader(traindata, batch_size=40000, shuffle=True) # Create DataLoader for training set\n",
    "valloader = DataLoader(valdata, batch_size=10000, shuffle=True) # Create DataLoader for validation set\n",
    "testloader = DataLoader(testdata, batch_size=10000, shuffle=True) # Create DataLoader for test set\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "train_history = []\n",
    "test_history = []\n",
    "'''\n",
    "for i in range(10):\n",
    "    BaselineModel = Baseline().to(device)\n",
    "    BaselineOptimizer = optim.Adam(BaselineModel.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    Baseline_train_history = fit(BaselineModel, BaselineOptimizer, \n",
    "                                 dataloader['train'], dataloader['val'], dataloader['test'], \n",
    "                                 batch_size=40000, model_type='Baseline', epochs=50, device=device)\n",
    "    Baseline_test_history = evaluate(BaselineModel, dataloader['test'], model_type='Baseline', device=device)\n",
    "    train_history.append(Baseline_train_history[-6]['val_acc'])\n",
    "    test_history.append(Baseline_test_history['val_acc'])\n",
    "'''\n",
    "for i in range(10):\n",
    "    SemiStructuredModel = SemiStructuredNet(batch_size=40000, cf_dim=2, num_features=32).to(device)\n",
    "    SemiStructuredModelOptimizer = optim.Adam(SemiStructuredModel.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    SemiStructredModel_train_history = fit(SemiStructuredModel, SemiStructuredModelOptimizer, \n",
    "                                        dataloader['train'], dataloader['val'], dataloader['test'], \n",
    "                                        batch_size=40000, model_type='SSN', cf_dim=2, num_features=32, epochs=50, device=device)\n",
    "    SemiStructured_test = evaluate(SemiStructuredModel, dataloader['test'], model_type='SSN', device=device)\n",
    "    print(f'Testing Loss: {SemiStructured_test[\"val_loss\"]:.4f}, Testing Accuracy: {SemiStructured_test[\"val_acc\"]:.2%}')\n",
    "    train_history.append(SemiStructredModel_train_history[-6]['val_acc'])\n",
    "    test_history.append(SemiStructured_test['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_mean = np.mean(train_history)\n",
    "train_acc_ci = stats.t.interval(0.95, len(train_history)-1, loc=train_acc_mean, scale=stats.sem(train_history))\n",
    "test_acc_mean = np.mean(test_history)\n",
    "test_acc_ci = stats.t.interval(0.95, len(test_history)-1, loc=test_acc_mean, scale=stats.sem(test_history))\n",
    "print(f'Validation accuracy: {train_acc_mean:.4f} ± {train_acc_ci[1]-train_acc_mean:.4f}')\n",
    "print(f'Test accuracy: {test_acc_mean:.4f} ± {test_acc_ci[1]-test_acc_mean:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of SSN on different test environments\n",
    "device = torch.device(\"cpu\")\n",
    "p_e = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "train_history = {} # Dictionary for storing training history for different batch sizes\n",
    "test_history = {} # Dictionary for storing test results for different batch sizes\n",
    "preparation = prepare_data() # Prepare data for given batch size\n",
    "for run in range(5):\n",
    "    train_history[run], test_history[run] = {}, {} # Initialize dictionaries for current run\n",
    "    train_history[run]['Baseline'], test_history[run]['Baseline'] = [], [] # Initialize lists for storing training history and test results for Baseline model\n",
    "    train_history[run]['SSN'], test_history[run]['SSN'] = [], [] # Initialize lists for storing training history and test results for Semi-Structured model\n",
    "    print(f'Run {run+1}...')\n",
    "    for p in p_e:\n",
    "        dataloader = create_dataloader(**preparation, batch_size=100, p_test=p) # Create dataloaders for given batch size\n",
    "        # Create and train models\n",
    "        SemiStructuredModel = SemiStructuredNet(batch_size=100, cf_dim=2, num_features=32).to(device)\n",
    "        SemiStructuredModelOptimizer = optim.Adam(SemiStructuredModel.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "        print(f'Training Semi-Structured model with p = {p}...')\n",
    "        train_history[run]['SSN'].append(fit(SemiStructuredModel, SemiStructuredModelOptimizer, \n",
    "                                             dataloader['train'], dataloader['val'], dataloader['test'], \n",
    "                                             batch_size=100, model_type='SSN', cf_dim=2, num_features=32, epochs=50, device=device))\n",
    "        test_history[run]['SSN'].append(evaluate(SemiStructuredModel, dataloader['test'], model_type='SSN', device=device))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results for different environments for plotting\n",
    "p_results = pd.DataFrame(columns=['p', 'Final Train Accuracy', 'Test Accuracy'])\n",
    "p_train_mean = []\n",
    "p_train_ci = []\n",
    "p_test_mean = []\n",
    "p_test_ci = []\n",
    "for i, p in enumerate(p_e):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    for run in range(5):\n",
    "        train_losses.append(train_history[run]['SSN'][i][-6]['train_loss'])\n",
    "        test_losses.append(test_history[run]['SSN'][i]['val_loss'])\n",
    "        train_accs.append(train_history[run]['SSN'][i][-6]['val_acc'])\n",
    "        test_accs.append(test_history[run]['SSN'][i]['val_acc'])\n",
    "    # Calculate mean and 95% confidence interval for train and test losses and accuracies\n",
    "    train_loss_mean = np.mean(train_losses)\n",
    "    train_loss_ci = stats.t.interval(0.95, len(train_losses)-1, loc=train_loss_mean, scale=stats.sem(train_losses))\n",
    "    test_loss_mean = np.mean(test_losses)\n",
    "    test_loss_ci = stats.t.interval(0.95, len(test_losses)-1, loc=test_loss_mean, scale=stats.sem(test_losses))\n",
    "    train_acc_mean = np.mean(train_accs)\n",
    "    train_acc_ci = stats.t.interval(0.95, len(train_accs)-1, loc=train_acc_mean, scale=stats.sem(train_accs))\n",
    "    test_acc_mean = np.mean(test_accs)\n",
    "    test_acc_ci = stats.t.interval(0.95, len(test_accs)-1, loc=test_acc_mean, scale=stats.sem(test_accs))\n",
    "    # Add results to table\n",
    "    p_results = p_results.append({'p': p, \n",
    "                                          'Final Train Accuracy': f'{train_acc_mean:.2%}' + u\"\\u00B1\" + f'{train_acc_ci[1]-train_acc_mean:.2%}', \n",
    "                                          'Test Accuracy': f'{test_acc_mean:.2%}' + u\"\\u00B1\" + f'{test_acc_ci[1]-test_acc_mean:.2%}'}, ignore_index=True)\n",
    "    p_train_mean.append(train_acc_mean)\n",
    "    p_train_ci.append(train_acc_ci[1]-train_acc_mean)\n",
    "    p_test_mean.append(test_acc_mean)\n",
    "    p_test_ci.append(test_acc_ci[1]-test_acc_mean)\n",
    "p_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "plt.plot(p_e, p_train_mean, '-r')\n",
    "plt.plot(p_e, p_test_mean, '-g')\n",
    "plt.vlines(np.array(p_e), np.array(p_train_mean)-np.array(p_train_ci), np.array(p_train_mean)+np.array(p_train_ci), colors='r', linestyles='dashed')\n",
    "plt.vlines(np.array(p_e), np.array(p_test_mean)-np.array(p_test_ci), np.array(p_test_mean)+np.array(p_test_ci), colors='g', linestyles='dashed')\n",
    "plt.legend(['Validation', 'Test'], frameon=False)\n",
    "plt.xlabel(r'$p^{e}$')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(f'plots/ColoredMNIST/p_e.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "# Compute SHAP values for Baseline and Semi-Structured Net model:\n",
    "###############################################################################################################\n",
    "device = torch.device(\"cpu\")\n",
    "batch_size = 100 # Set batch size for SHAP computation\n",
    "model_Baseline = Baseline().to(device) # Create model instance\n",
    "model_Baseline.load_state_dict(torch.load('fitted_models/ColoredMNIST/best-model-Baseline-100.pth')) # Load fitted model\n",
    "model_Baseline.eval() # Set model to evaluation mode\n",
    "model_SemiStructured = SemiStructuredNet(batch_size=100, cf_dim=2, num_features=32).to(device) # Create model instance\n",
    "model_SemiStructured.load_state_dict(torch.load('fitted_models/ColoredMNIST/best-model-SSN-100.pth')) # Load fitted model\n",
    "model_SemiStructured.eval() # Set model to evaluation mode\n",
    "# Create background sample\n",
    "dataloader = create_dataloader(**preparation, batch_size=100)\n",
    "background_sample = next(iter(dataloader['test']))\n",
    "background_images = background_sample['images'].to(device)\n",
    "background_labels = background_sample['labels'].to(device)\n",
    "background_colors = background_sample['colors'].to(device)\n",
    "# Set confounders for Semi-Structured Net model\n",
    "with torch.no_grad():\n",
    "    model_SemiStructured.set_confounders(background_colors, cf_dim=2, device=device)\n",
    "background_images = background_images.view(background_images.shape[0], -1)\n",
    "explainer_Baseline = shap.DeepExplainer(model_Baseline, background_images) # Create explainer instance for Baseline model\n",
    "explainer_SemiStructured = shap.DeepExplainer(model_SemiStructured, background_images) # Create explainer instance for Semi-Structured Net model\n",
    "# Create test sample\n",
    "test_sample = next(iter(dataloader['test']))\n",
    "test_images = test_sample['images'].to(device)\n",
    "test_labels = test_sample['labels'].to(device)\n",
    "test_colors = test_sample['colors'].to(device)\n",
    "# Set confounders for Semi-Structured Net model by using the confounder of the background and concatenating it with the confounder of the test sample\n",
    "with torch.no_grad():\n",
    "    model_SemiStructured.cfs = nn.Parameter(torch.cat((\n",
    "        torch.cat((torch.ones((len(background_colors), 1)), torch.Tensor(background_colors)), dim=1).to(device),\n",
    "        torch.cat((torch.ones((len(test_colors), 1)), torch.Tensor(test_colors)), dim=1).to(device)\n",
    "    )), requires_grad=False)\n",
    "test_images = test_images.view(background_images.shape[0], -1)\n",
    "shap_values_Baseline = explainer_Baseline.shap_values(test_images) # Compute SHAP values for Baseline model\n",
    "shap_values_SemiStructured = explainer_SemiStructured.shap_values(test_images) # Compute SHAP values for Semi-Structured Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots of SHAP values for Baseline model and save to file\n",
    "plt.ioff() # Turn interactive mode off to prevent plots from being displayed\n",
    "shap.image_plot(np.reshape(shap_values_Baseline[:,:196], (len(test_images), 14, 14, 1))[:10], \n",
    "                -np.reshape(test_images.numpy()[:,:196], (len(test_images), 14, 14, 1))[:10], show=False)\n",
    "plt.savefig(f'plots/ColoredMNIST/shap_values_Baseline-{batch_size}_channel0.png', dpi=1200, bbox_inches='tight')\n",
    "shap.image_plot(np.reshape(shap_values_Baseline[:,196:], (len(test_images), 14, 14, 1))[:10], \n",
    "                -np.reshape(test_images.numpy()[:,196:], (len(test_images), 14, 14, 1))[:10], show=False)\n",
    "plt.savefig(f'plots/ColoredMNIST/shap_values_Baseline-{batch_size}_channel1.png', dpi=1200, bbox_inches='tight')\n",
    "shap.image_plot(np.reshape(shap_values_Baseline[:,:196]+shap_values_Baseline[:,196:], (len(test_images), 14, 14, 1))[:10], \n",
    "                -np.reshape(test_images.numpy()[:,:196]+test_images.numpy()[:,196:], (len(test_images), 14, 14, 1))[:10], \n",
    "                labels=test_labels[:10].numpy().astype(int), \n",
    "                show=False)\n",
    "plt.savefig(f'plots/ColoredMNIST/shap_values_Baseline-{batch_size}_both_channels.png', dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display plots of SHAP values for Baseline model in a grid\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "gs = gridspec.GridSpec(nrows=1, ncols=3)\n",
    "ax0 = fig.add_subplot(gs[0, 1])\n",
    "ax0.imshow(plt.imread(f'plots/ColoredMNIST/shap_values_Baseline-{batch_size}_channel0.png'))\n",
    "ax1 = fig.add_subplot(gs[0, 2])\n",
    "ax1.imshow(plt.imread(f'plots/ColoredMNIST/shap_values_Baseline-{batch_size}_channel1.png'))\n",
    "ax2 = fig.add_subplot(gs[0, 0])\n",
    "ax2.imshow(plt.imread(f'plots/ColoredMNIST/shap_values_Baseline-{batch_size}_both_channels.png'))\n",
    "ax0.axis('off')\n",
    "ax1.axis('off')\n",
    "ax2.axis('off')\n",
    "plt.savefig(f'plots/ColoredMNIST/shap_values_Baseline-{batch_size}_full.pdf', dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots of SHAP values for Semi-Structured Net model and save to file\n",
    "plt.ioff() # Turn interactive mode off to prevent plots from being displayed\n",
    "shap.image_plot(np.reshape(shap_values_SemiStructured[:,:196], (len(test_images), 14, 14, 1))[:10], \n",
    "                -np.reshape(test_images.numpy()[:,:196], (len(test_images), 14, 14, 1))[:10], show=False)\n",
    "plt.savefig(f'plots/ColoredMNIST/shap_values_SemiStructured-{batch_size}_channel0.pdf', dpi=1200, bbox_inches='tight')\n",
    "shap.image_plot(np.reshape(shap_values_SemiStructured[:,196:], (len(test_images), 14, 14, 1))[:10], \n",
    "                -np.reshape(test_images.numpy()[:,196:], (len(test_images), 14, 14, 1))[:10], show=False)\n",
    "plt.savefig(f'plots/ColoredMNIST/shap_values_SemiStructured-{batch_size}_channel1.pdf', dpi=1200, bbox_inches='tight')\n",
    "shap.image_plot(np.reshape(shap_values_SemiStructured[:,:196]+shap_values_SemiStructured[:,196:], (len(test_images), 14, 14, 1))[:10], \n",
    "                -np.reshape(test_images.numpy()[:,:196]+test_images.numpy()[:,196:], (len(test_images), 14, 14, 1))[:10], \n",
    "                labels=test_labels[:10].numpy().astype(int), \n",
    "                show=False)\n",
    "plt.savefig(f'plots/ColoredMNIST/shap_values_SemiStructured-{batch_size}_both_channels.pdf', dpi=1200, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display plots of SHAP values for Semi-Structured Net model in a grid\n",
    "fig = plt.figure(figsize=(12, 16))\n",
    "gs = gridspec.GridSpec(nrows=1, ncols=3)\n",
    "ax0 = fig.add_subplot(gs[0, 1])\n",
    "ax0.imshow(plt.imread(f'plots/ColoredMNIST/shap_values_SemiStructured-{batch_size}_channel0.png'))\n",
    "ax1 = fig.add_subplot(gs[0, 2])\n",
    "ax1.imshow(plt.imread(f'plots/ColoredMNIST/shap_values_SemiStructured-{batch_size}_channel1.png'))\n",
    "ax2 = fig.add_subplot(gs[0, 0])\n",
    "ax2.imshow(plt.imread(f'plots/ColoredMNIST/shap_values_SemiStructured-{batch_size}_both_channels.png'))\n",
    "ax0.axis('off')\n",
    "ax1.axis('off')\n",
    "ax2.axis('off')\n",
    "plt.savefig(f'plots/ColoredMNIST/shap_values_SemiStructured-{batch_size}_full.png', dpi=1200, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
